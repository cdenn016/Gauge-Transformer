\documentclass[11pt,a4paper]{article}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{bbm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{xcolor}

\geometry{margin=1in}

% ============================================================================
% THEOREM ENVIRONMENTS
% ============================================================================
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{axiom}{Axiom}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% ============================================================================
% CUSTOM COMMANDS
% ============================================================================
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\pp}{\partial}
\newcommand{\T}{\mathsf{T}}

% ============================================================================
% TITLE
% ============================================================================
\title{%
    \textbf{From Information Geometry to Continuous Thought Machines:}\\[0.5em]
    \large A Rigorous Derivation from First Principles
}

\author{
    Gauge-Transformer Project\\
    \texttt{github.com/cdenn016/Gauge-Transformer}
}

\date{January 2026}

% ============================================================================
% DOCUMENT
% ============================================================================
\begin{document}

\maketitle

\begin{abstract}
We establish a rigorous mathematical connection between information-geometric gauge theory and the Continuous Thought Machine (CTM) architecture. Starting from five first principles---statistical manifolds, the Fisher-Rao metric, proper time, gauge covariance, and free energy minimization---we derive the key structural components of CTMs: neuron-level models (NLMs), synchronization representations, and adaptive computation. The central mathematical result is that temporal synchronization between neurons corresponds to KL-based belief alignment through the mutual information identity $I(X;Y) = \KL(p_{XY} \| p_X p_Y)$. We identify the precise assumptions required for exact versus approximate correspondence and propose a ``Gauge-Theoretic CTM'' that implements these principles explicitly.
\end{abstract}

\tableofcontents
\newpage

% ============================================================================
\section{Introduction}
% ============================================================================

The Continuous Thought Machine (CTM) \cite{darlow2025ctm} represents a significant departure from standard transformer architectures by introducing:
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Neuron-level models (NLMs)}: Each neuron has private weights processing its activation history
    \item \textbf{Synchronization representations}: Pairwise temporal correlations as the latent space
    \item \textbf{Adaptive computation}: Internal ``ticks'' decoupled from input, halting based on confidence
\end{enumerate}

Independently, gauge-theoretic approaches to neural computation have proposed that:
\begin{enumerate}[label=(\roman*)]
    \item Beliefs are points on a \textbf{statistical manifold} with the Fisher-Rao metric
    \item \textbf{Proper time} is the information-theoretic arc length: ``a difference which makes a difference''
    \item Inter-agent coupling uses \textbf{KL divergence} with gauge-covariant transport
    \item Dynamics minimize a \textbf{variational free energy} functional
\end{enumerate}

This paper establishes that these frameworks are deeply connected: CTM's architecture can be \emph{derived} from information-geometric first principles. The key mathematical bridge is the identity relating mutual information to KL divergence, which connects correlation-based synchronization to distribution-based alignment.

% ============================================================================
\section{First Principles}
\label{sec:axioms}
% ============================================================================

We begin with five axioms that define the mathematical framework.

\begin{axiom}[Statistical Manifold]
\label{ax:manifold}
The state space of computation is a statistical manifold
\[
\M = \{q_\theta : \theta \in \Theta \subseteq \R^d\}
\]
where each $q_\theta$ is a probability distribution over some sample space $\mathcal{X}$.
\end{axiom}

\begin{axiom}[Fisher-Rao Metric]
\label{ax:fisher}
The natural Riemannian metric on $\M$ is the Fisher information matrix:
\begin{equation}
\label{eq:fisher}
G_{\mu\nu}(\theta) = \E_{q_\theta}\left[\frac{\pp \log q_\theta}{\pp \theta^\mu} \cdot \frac{\pp \log q_\theta}{\pp \theta^\nu}\right]
\end{equation}
For multivariate Gaussians $q = \N(\mu, \Sigma)$ with $\theta = (\mu, \Sigma)$, this gives:
\begin{equation}
\label{eq:fisher_gaussian}
G = \begin{pmatrix} \Sigma^{-1} & 0 \\ 0 & \frac{1}{2}(\Sigma^{-1} \otimes \Sigma^{-1}) \end{pmatrix}
\end{equation}
\end{axiom}

\begin{axiom}[Proper Time]
\label{ax:proper_time}
Time is defined as the arc length on $\M$:
\begin{equation}
\label{eq:proper_time}
\boxed{\dd\tau^2 = \dd\theta^\T G(\theta) \dd\theta}
\end{equation}
For Gaussians with fixed covariance:
\begin{equation}
\label{eq:proper_time_gaussian}
\dd\tau^2 = \dd\mu^\T \Sigma^{-1} \dd\mu
\end{equation}
This satisfies $\KL(q \| q + \dd q) \approx \frac{1}{2}\dd\tau^2$ to second order.
\end{axiom}

\begin{axiom}[Gauge Covariance]
\label{ax:gauge}
Physical observables are invariant under gauge transformations $g \in \mathcal{G}$ acting on the manifold. For rotation gauge group $\mathcal{G} = SO(N)$:
\begin{align}
\mu &\mapsto g\mu \\
\Sigma &\mapsto g\Sigma g^\T
\end{align}
KL divergence is gauge-invariant: $\KL(q_i \| \Omega_{ij}[q_j])$ is independent of gauge frame, where $\Omega_{ij}$ is the parallel transport operator.
\end{axiom}

\begin{axiom}[Free Energy Minimization]
\label{ax:free_energy}
Dynamics minimize the variational free energy functional:
\begin{equation}
\label{eq:free_energy}
\F[q] = \underbrace{\alpha \cdot \KL(q \| p)}_{\text{compression}} + \underbrace{\langle H \rangle_q}_{\text{expected cost}} + \underbrace{\lambda \sum_{i,j} \beta_{ij} \KL(q_i \| \Omega_{ij}[q_j])}_{\text{belief alignment}}
\end{equation}
where $p$ is a prior, $H$ is a task-dependent Hamiltonian, and $\beta_{ij} = \softmax_j(-\KL(q_i \| q_j)/\kappa)$ are attention weights.
\end{axiom}

% ============================================================================
\section{Derivation of CTM Architecture}
\label{sec:derivation}
% ============================================================================

We now derive each component of the CTM architecture from the axioms.

% ----------------------------------------------------------------------------
\subsection{Neuron-Level Models from Fisher Metric}
% ----------------------------------------------------------------------------

\begin{theorem}[NLM Structure]
\label{thm:nlm}
If each computational unit $i$ maintains a distribution $q_i$ on a statistical manifold with unit-specific Fisher metric $G_i$, then optimal linear parameterization requires unit-specific weight matrices $W_i$ satisfying:
\begin{equation}
G_i = W_i^\T W_i
\end{equation}
\end{theorem}

\begin{proof}
Consider a computational unit $i$ that maps inputs $x \in \R^n$ to a distribution $q_i(z; x) = \N(W_i x, \Sigma_i)$ over latent states $z$.

\textbf{Step 1:} By Axiom~\ref{ax:fisher}, the Fisher information with respect to the mean parameter is $\Sigma_i^{-1}$.

\textbf{Step 2:} The Fisher information with respect to the \emph{input} $x$ is obtained by the chain rule. For $\mu = W_i x$:
\begin{equation}
G_i^{(x)} = \frac{\pp \mu}{\pp x}^\T G_i^{(\mu)} \frac{\pp \mu}{\pp x} = W_i^\T \Sigma_i^{-1} W_i
\end{equation}

\textbf{Step 3:} If $\Sigma_i = I$ (unit covariance), then $G_i^{(x)} = W_i^\T W_i$.

\textbf{Step 4:} For each unit to have its own ``sensitivity'' to inputs---its own local metric on input space---we require unit-specific $W_i$.

This is precisely the NLM structure: in CTM, each neuron $d$ has private weights $W_d$ that process its input history. The weights implicitly define the neuron's local metric on activation space.
\end{proof}

\begin{corollary}[Precision Dynamics]
\label{cor:precision}
High-precision units (large eigenvalues of $G_i = W_i^\T W_i$) experience faster proper time for fixed input changes:
\begin{equation}
\dd\tau_i = \|dx\|_{G_i} = \sqrt{\dd x^\T W_i^\T W_i \dd x}
\end{equation}
\end{corollary}

\begin{remark}
In the gauge-theoretic framework, the covariance $\Sigma_i$ represents uncertainty, and its inverse $\Sigma_i^{-1}$ is the precision. The NLM weights $W_d$ in CTM play an analogous role: they determine how sensitively each neuron responds to its input history. Neurons with large $\|W_d\|$ are ``high-precision'' and experience rapid proper time---small input changes produce large activation changes.
\end{remark}

% ----------------------------------------------------------------------------
\subsection{Synchronization from KL Divergence}
% ----------------------------------------------------------------------------

The key mathematical result connecting the two frameworks involves mutual information.

\begin{lemma}[Mutual Information as KL Divergence]
\label{lem:mi_kl}
For any joint distribution $p_{XY}$ with marginals $p_X$, $p_Y$:
\begin{equation}
\label{eq:mi_kl}
I(X; Y) = \KL(p_{XY} \| p_X \otimes p_Y)
\end{equation}
Mutual information \emph{is} the KL divergence from the joint to the product of marginals.
\end{lemma}

\begin{proof}
By definition of mutual information and KL divergence:
\begin{align}
I(X; Y) &= H(X) + H(Y) - H(X, Y) \\
&= \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)} \\
&= \KL(p_{XY} \| p_X \otimes p_Y)
\end{align}
\end{proof}

\begin{lemma}[Correlation and Mutual Information for Gaussians]
\label{lem:corr_mi}
For jointly Gaussian random variables $(X, Y)$ with correlation coefficient $\rho$:
\begin{equation}
\label{eq:corr_mi}
I(X; Y) = -\frac{1}{2}\log(1 - \rho^2)
\end{equation}
\end{lemma}

\begin{proof}
For bivariate Gaussian with covariance matrix:
\[
\Sigma = \begin{pmatrix} \sigma_X^2 & \rho\sigma_X\sigma_Y \\ \rho\sigma_X\sigma_Y & \sigma_Y^2 \end{pmatrix}
\]
we have $\det(\Sigma) = \sigma_X^2 \sigma_Y^2 (1 - \rho^2)$.

The entropies are:
\begin{align}
H(X) &= \frac{1}{2}\log(2\pi e \sigma_X^2) \\
H(Y) &= \frac{1}{2}\log(2\pi e \sigma_Y^2) \\
H(X,Y) &= \frac{1}{2}\log((2\pi e)^2 \det(\Sigma))
\end{align}

Therefore:
\begin{align}
I(X;Y) &= H(X) + H(Y) - H(X,Y) \\
&= \frac{1}{2}\log\frac{(2\pi e)^2 \sigma_X^2 \sigma_Y^2}{(2\pi e)^2 \sigma_X^2 \sigma_Y^2 (1-\rho^2)} \\
&= -\frac{1}{2}\log(1 - \rho^2)
\end{align}
\end{proof}

\begin{theorem}[Synchronization as Information-Geometric Alignment]
\label{thm:sync}
Let $x_i(t), x_j(t)$ be activation trajectories of neurons $i, j$ over time $t \in \{1, \ldots, T\}$. Define:
\begin{itemize}
    \item Synchronization: $S_{ij} = \mathrm{corr}(x_i, x_j)$ (temporal correlation)
    \item KL-based alignment: $\beta_{ij} = \softmax_j(-\KL(q_i \| q_j)/\kappa)$
\end{itemize}
Then $S_{ij}$ and $\beta_{ij}$ are monotonically related through mutual information.
\end{theorem}

\begin{proof}
\textbf{Step 1: Temporal distributions.}
View the histories $(x_i(1), \ldots, x_i(T))$ as samples from temporal distributions. Define the joint temporal distribution $p_{ij}$ over $(x_i, x_j)$ pairs across time.

\textbf{Step 2: Synchronization measures dependence.}
The synchronization $S_{ij} = \rho_{ij}$ measures linear dependence. By Lemma~\ref{lem:corr_mi}, for Gaussian activations:
\begin{equation}
I(x_i; x_j) = -\frac{1}{2}\log(1 - S_{ij}^2)
\end{equation}

\textbf{Step 3: Mutual information is KL divergence.}
By Lemma~\ref{lem:mi_kl}:
\begin{equation}
I(x_i; x_j) = \KL(p_{ij} \| p_i \otimes p_j)
\end{equation}

\textbf{Step 4: Inverting the relationship.}
From \eqref{eq:corr_mi}:
\begin{equation}
S_{ij}^2 = 1 - \exp(-2 I(x_i; x_j)) = 1 - \exp(-2\KL(p_{ij} \| p_i \otimes p_j))
\end{equation}

For small mutual information (weak dependence):
\begin{equation}
|S_{ij}| \approx \sqrt{2 I(x_i; x_j)} = \sqrt{2\KL(p_{ij} \| p_i \otimes p_j)}
\end{equation}

\textbf{Step 5: Softmax form.}
The gauge-theoretic attention uses:
\begin{equation}
\beta_{ij} = \softmax_j\left(-\frac{\KL(q_i \| \Omega_{ij}[q_j])}{\kappa}\right)
\end{equation}

CTM's synchronization, when exponentiated and normalized, has the form:
\begin{equation}
S_{ij}^{\text{normalized}} \propto \exp(f(|S_{ij}|))
\end{equation}

Both frameworks implement: \textbf{attention weight = softmax of (negative) information distance}.
\end{proof}

\begin{tcolorbox}[colback=blue!5,colframe=blue!50,title=Key Mathematical Identity]
The bridge between correlation-based synchronization and KL-based alignment:
\begin{equation}
\boxed{I(X;Y) = \KL(p_{XY} \| p_X p_Y) \approx \frac{\rho^2}{2} \quad \text{for small } \rho}
\end{equation}
High synchronization ($\rho \to 1$) $\Leftrightarrow$ High mutual information $\Leftrightarrow$ Low divergence from joint to product.
\end{tcolorbox}

% ----------------------------------------------------------------------------
\subsection{Adaptive Computation from Proper Time}
% ----------------------------------------------------------------------------

\begin{theorem}[Adaptive Halting from Proper Time]
\label{thm:adaptive}
If computation halts when the rate of proper time accumulation falls below a threshold:
\begin{equation}
\frac{\dd\tau}{\dd t} < \epsilon \quad \text{for } k \text{ consecutive ticks}
\end{equation}
then:
\begin{enumerate}[label=(\alph*)]
    \item Simple inputs (rapid belief convergence) halt early
    \item Complex inputs (prolonged belief evolution) halt late
    \item High-precision units process information faster
\end{enumerate}
\end{theorem}

\begin{proof}
\textbf{(a) Simple inputs:}
For simple inputs, the belief $\mu(t)$ quickly converges to a fixed point. Thus $\dd\mu_t \to 0$, giving:
\begin{equation}
\frac{\dd\tau}{\dd t} = \sqrt{\dd\mu_t^\T \Sigma^{-1} \dd\mu_t} \to 0
\end{equation}
The halting criterion is satisfied early.

\textbf{(b) Complex inputs:}
For complex inputs requiring extended reasoning, the belief continues to evolve: $\dd\mu_t$ remains significant. Thus $\dd\tau/\dd t$ stays above threshold, and computation continues.

\textbf{(c) Precision dependence:}
For fixed $|\dd\mu|$, the proper time increment scales with precision:
\begin{equation}
\dd\tau = |\dd\mu| \cdot \sqrt{\lambda_{\max}(\Sigma^{-1})}
\end{equation}
High-precision units (large $\Sigma^{-1}$) accumulate more proper time per external tick. They ``think faster'' in the sense that each update carries more information-theoretic weight.
\end{proof}

\begin{remark}[Connection to CTM Halting]
CTM halts based on a combination of:
\begin{enumerate}
    \item Lowest classification loss (task performance)
    \item Highest certainty: $1 - H(\text{output})$ (confidence)
\end{enumerate}

In information-geometric terms, certainty corresponds to precision:
\begin{equation}
\text{Certainty} \propto |\Sigma^{-1}| = \text{precision}
\end{equation}

When $\Sigma^{-1}$ is large and stable, $\dd\tau$ becomes small because the system is confident and no longer changing significantly. This is equivalent to the CTM halting condition.
\end{remark}

% ----------------------------------------------------------------------------
\subsection{Free Energy and CTM Loss}
% ----------------------------------------------------------------------------

\begin{theorem}[Equivalence of Objectives]
\label{thm:loss}
Under appropriate identifications, the gauge-theoretic free energy functional is structurally equivalent to CTM's implicit objective.
\end{theorem}

\begin{proof}
\textbf{Gauge-theoretic free energy:}
\begin{equation}
\F = \underbrace{\alpha \cdot \KL(q \| p)}_{\text{compression}} + \underbrace{\sum_{i,j} \beta_{ij} \KL(q_i \| \Omega_{ij}[q_j])}_{\text{alignment}} + \underbrace{\mathcal{L}_{\text{task}}}_{\text{prediction}}
\end{equation}

\textbf{CTM loss:}
\begin{equation}
\mathcal{L}_{\text{CTM}} = \mathcal{L}_{\text{task}} + \lambda \cdot H(\text{output})
\end{equation}

\textbf{Identification 1: Compression.}
The KL term $\KL(q \| p)$ encourages beliefs to stay close to priors, preventing overconfident representations. CTM's entropy term serves analogously: it regularizes the output distribution.

\textbf{Identification 2: Alignment.}
The belief alignment term creates pressure for coherent representations. CTM achieves this \emph{implicitly}: synchronization is used as a representation for prediction. Gradients from $\mathcal{L}_{\text{task}}$ through the synchronization $\to$ prediction pathway create implicit alignment pressure.

Concretely, if the prediction depends on synchronization:
\begin{equation}
\hat{y} = f(S) = f(\{S_{ij}\}_{i,j})
\end{equation}
then:
\begin{equation}
\frac{\pp \mathcal{L}}{\pp x_k} = \sum_{i,j} \frac{\pp \mathcal{L}}{\pp S_{ij}} \cdot \frac{\pp S_{ij}}{\pp x_k}
\end{equation}

This gradient encourages activation patterns that produce ``useful'' synchronization---functionally equivalent to the explicit alignment term.
\end{proof}

% ============================================================================
\section{Summary of Correspondences}
\label{sec:summary}
% ============================================================================

\begin{table}[h]
\centering
\caption{Correspondence between Gauge Theory and CTM}
\label{tab:correspondence}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Gauge Theory} & \textbf{CTM} & \textbf{Rigor} \\
\midrule
Statistical manifold $\M$ & State space of neurons & Exact \\
Fisher metric $G_i = W_i^\T W_i$ & NLM private weights $W_d$ & Exact \\
Proper time $\dd\tau^2 = \dd\mu^\T \Sigma^{-1} \dd\mu$ & Adaptive iteration count & Approximate \\
$\KL(q_i \| \Omega_{ij}[q_j])$ & Synchronization $S_{ij}$ & Approximate \\
$\beta_{ij} = \softmax(-\KL/\kappa)$ & Attention from sync & Structural \\
Free energy $\F$ & CTM loss (implicit) & Structural \\
Gauge covariance & Correlation invariance & Implicit \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
\section{Gaps and Required Assumptions}
\label{sec:gaps}
% ============================================================================

We identify five gaps between exact correspondence and the actual CTM implementation.

% ----------------------------------------------------------------------------
\subsection{Gap 1: Gaussian Assumption}
% ----------------------------------------------------------------------------

\textbf{Required:} Neuron activations are approximately Gaussian.

\textbf{Reality:} CTM activations may be non-Gaussian due to nonlinearities (ReLU, etc.).

\textbf{Mitigation:} The Fisher metric exists for any exponential family distribution:
\begin{equation}
p(x; \theta) = h(x) \exp(\theta^\T T(x) - A(\theta))
\end{equation}
with Fisher information $G(\theta) = \nabla^2 A(\theta)$. Gaussianity simplifies calculations but is not essential to the framework.

\textbf{Generalization:} For non-Gaussian activations, replace correlation with mutual information estimated via:
\begin{equation}
\hat{I}(X; Y) = \hat{H}(X) + \hat{H}(Y) - \hat{H}(X, Y)
\end{equation}
using entropy estimators (e.g., k-NN based).

% ----------------------------------------------------------------------------
\subsection{Gap 2: Temporal vs. Instantaneous}
% ----------------------------------------------------------------------------

\textbf{Issue:} The gauge theory is formulated for instantaneous beliefs $q(t)$; CTM operates on temporal histories.

\textbf{Bridge:} Define temporal distributions over length-$T$ trajectories. The Fisher metric on trajectory space becomes:
\begin{equation}
G^{\text{temporal}} = \frac{1}{T} \sum_{t=1}^T G(\theta(t))
\end{equation}

For stationary processes, this reduces to the instantaneous Fisher metric multiplied by effective degrees of freedom.

% ----------------------------------------------------------------------------
\subsection{Gap 3: Synchronization $\neq$ KL (Exactly)}
% ----------------------------------------------------------------------------

\textbf{Correlation:}
\begin{equation}
\rho_{ij} = \frac{\mathrm{cov}(x_i, x_j)}{\sigma_i \sigma_j}
\end{equation}

\textbf{KL-based alignment:}
\begin{equation}
\beta_{ij} = \softmax\left(-\frac{\KL(q_i \| q_j)}{\kappa}\right)
\end{equation}

These are \textbf{not identical}, but they are \textbf{monotonically related} for Gaussians. The key relationships:
\begin{align}
\KL(\N(\mu_i, \Sigma_i) \| \N(\mu_j, \Sigma_j)) &= \frac{1}{2}\left[\tr(\Sigma_j^{-1}\Sigma_i) + (\mu_j - \mu_i)^\T \Sigma_j^{-1}(\mu_j - \mu_i) - K + \ln\frac{|\Sigma_j|}{|\Sigma_i|}\right]
\end{align}

When means track each other (high correlation $\Rightarrow$ $\mu_i \approx \mu_j$), the quadratic term vanishes and KL is small, so $\beta_{ij}$ is large.

% ----------------------------------------------------------------------------
\subsection{Gap 4: Gauge Transport}
% ----------------------------------------------------------------------------

\textbf{Gauge theory:} Explicit transport operator $\Omega_{ij}$ maps between gauge frames:
\begin{equation}
\Sigma_j^{\text{transported}} = \Omega_{ij} \Sigma_j \Omega_{ij}^\T
\end{equation}

\textbf{CTM:} No explicit gauge structure.

\textbf{Resolution:} CTM's synchronization (correlation) is \emph{already gauge-invariant} because correlation is invariant under:
\begin{itemize}
    \item Shifts: $x \mapsto x + c$ (correlation unchanged)
    \item Scalings: $x \mapsto \alpha x$ (correlation unchanged)
    \item Rotations: $x \mapsto Rx$ (correlation magnitude preserved)
\end{itemize}

Thus CTM \emph{implicitly} has gauge covariance through the intrinsic properties of correlation.

% ----------------------------------------------------------------------------
\subsection{Gap 5: Explicit vs. Implicit Optimization}
% ----------------------------------------------------------------------------

\textbf{Gauge theory:} Explicitly minimize $\F$ via gradient descent:
\begin{equation}
\frac{\dd\theta}{\dd\tau} = -G^{-1} \nabla_\theta \F
\end{equation}

\textbf{CTM:} Synchronization is a \emph{representation}, not a loss term.

\textbf{Resolution:} Gradient descent through the prediction pathway:
\begin{equation}
\frac{\pp \mathcal{L}}{\pp W_d} = \frac{\pp \mathcal{L}}{\pp \hat{y}} \cdot \frac{\pp \hat{y}}{\pp S} \cdot \frac{\pp S}{\pp a} \cdot \frac{\pp a}{\pp W_d}
\end{equation}
creates implicit pressure toward ``useful'' synchronization patterns. This is functionally equivalent to adding an explicit synchronization objective, but learned rather than prescribed.

% ============================================================================
\section{Proposed Architecture: Gauge-Theoretic CTM}
\label{sec:proposal}
% ============================================================================

Based on the derivations above, we propose explicit modifications to CTM that implement the information-geometric principles directly.

\subsection{Replace Correlation with KL-Based Synchronization}

Instead of:
\begin{equation}
S_{ij} = \mathrm{corr}(x_i, x_j)
\end{equation}

Use:
\begin{equation}
S_{ij}^{\text{gauge}} = \exp\left(-\frac{\KL(q_i \| \Omega_{ij}[q_j])}{\kappa}\right)
\end{equation}

where $q_i = \N(\bar{x}_i, \hat{\Sigma}_i)$ is estimated from the activation history.

\subsection{Replace Internal Ticks with Proper Time}

Instead of fixed iteration count, iterate until:
\begin{equation}
\Delta\tau = \sum_{d} \sqrt{\Delta\mu_d^\T \Sigma_d^{-1} \Delta\mu_d} < \epsilon
\end{equation}

where the sum is over neurons and $\Delta\mu_d$ is the change in neuron $d$'s activation.

\subsection{Add Explicit Gauge Transport}

For multi-head or multi-frame reasoning, include learnable transport operators:
\begin{equation}
\Omega_{ij} = \exp\left(\sum_a \omega_{ij}^a T_a\right) \in SO(K)
\end{equation}

where $T_a$ are generators of the rotation group and $\omega_{ij}^a$ are learned coefficients.

\subsection{Use Variational Free Energy as Loss}

Replace the CTM loss with:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{task}} + \alpha \sum_i \KL(q_i \| p_i) + \lambda \sum_{i,j} \beta_{ij} \KL(q_i \| \Omega_{ij}[q_j])
\end{equation}

This makes the information-geometric structure explicit and learnable.

% ============================================================================
\section{Conclusion}
\label{sec:conclusion}
% ============================================================================

We have established that the Continuous Thought Machine architecture can be rigorously derived from information-geometric first principles:

\begin{enumerate}
    \item \textbf{Neuron-Level Models} arise from requiring unit-specific Fisher metrics (Theorem~\ref{thm:nlm})

    \item \textbf{Synchronization} corresponds to KL-based belief alignment through the mutual information identity (Theorem~\ref{thm:sync})

    \item \textbf{Adaptive Computation} follows from proper time dynamics (Theorem~\ref{thm:adaptive})

    \item \textbf{The overall objective} is structurally equivalent to variational free energy minimization (Theorem~\ref{thm:loss})
\end{enumerate}

The key mathematical bridge is:
\begin{equation}
I(X;Y) = \KL(p_{XY} \| p_X p_Y)
\end{equation}

This identity connects correlation-based synchronization (CTM) to distribution-based alignment (gauge theory).

The derivation is exact for the NLM structure and softmax attention form, and approximate (requiring Gaussian + stationarity assumptions) for the synchronization correspondence and adaptive compute mechanism.

The proposed ``Gauge-Theoretic CTM'' would implement these principles explicitly, potentially offering:
\begin{itemize}
    \item More principled halting criteria based on information-theoretic proper time
    \item Richer synchronization representations via KL divergence with gauge transport
    \item Explicit free energy optimization with interpretable terms
\end{itemize}

This unification suggests that biological neural synchronization, gauge-theoretic belief dynamics, and modern neural architectures may share deep mathematical structure rooted in information geometry.

% ============================================================================
% REFERENCES
% ============================================================================
\begin{thebibliography}{99}

\bibitem{darlow2025ctm}
L.~Darlow, C.~Regan, S.~Risi, J.~Seely, and L.~Jones.
\newblock Continuous Thought Machines.
\newblock \emph{arXiv preprint arXiv:2505.05522}, 2025.

\bibitem{amari1998natural}
S.~Amari.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural Computation}, 10(2):251--276, 1998.

\bibitem{friston2010free}
K.~Friston.
\newblock The free-energy principle: a unified brain theory?
\newblock \emph{Nature Reviews Neuroscience}, 11(2):127--138, 2010.

\bibitem{ay2017information}
N.~Ay, J.~Jost, H.~V.~L\^e, and L.~Schwachh\"ofer.
\newblock \emph{Information Geometry}.
\newblock Springer, 2017.

\bibitem{tishby2000information}
N.~Tishby, F.~C.~Pereira, and W.~Bialek.
\newblock The information bottleneck method.
\newblock \emph{arXiv preprint physics/0004057}, 2000.

\end{thebibliography}

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Detailed Proof: Fisher Metric for Gaussians}
\label{app:fisher_proof}

For $q = \N(\mu, \Sigma)$, the log-likelihood is:
\begin{equation}
\log q(x) = -\frac{K}{2}\log(2\pi) - \frac{1}{2}\log|\Sigma| - \frac{1}{2}(x-\mu)^\T\Sigma^{-1}(x-\mu)
\end{equation}

The score functions are:
\begin{align}
\frac{\pp \log q}{\pp \mu} &= \Sigma^{-1}(x - \mu) \\
\frac{\pp \log q}{\pp \Sigma_{ab}} &= -\frac{1}{2}(\Sigma^{-1})_{ab} + \frac{1}{2}(\Sigma^{-1}(x-\mu)(x-\mu)^\T\Sigma^{-1})_{ab}
\end{align}

The Fisher information matrix blocks:
\begin{align}
G_{\mu\mu} &= \E[(\Sigma^{-1}(x-\mu))(\Sigma^{-1}(x-\mu))^\T] = \Sigma^{-1}\E[(x-\mu)(x-\mu)^\T]\Sigma^{-1} = \Sigma^{-1} \\
G_{\mu\Sigma} &= 0 \quad \text{(by symmetry)} \\
G_{\Sigma\Sigma} &= \frac{1}{2}(\Sigma^{-1} \otimes \Sigma^{-1})
\end{align}

\section{KL Divergence Between Gaussians}
\label{app:kl_gaussian}

For $q = \N(\mu_q, \Sigma_q)$ and $p = \N(\mu_p, \Sigma_p)$:
\begin{equation}
\KL(q \| p) = \frac{1}{2}\left[\log\frac{|\Sigma_p|}{|\Sigma_q|} + \tr(\Sigma_p^{-1}\Sigma_q) + (\mu_p - \mu_q)^\T\Sigma_p^{-1}(\mu_p - \mu_q) - K\right]
\end{equation}

To second order in small perturbations $\dd\mu$, $\dd\Sigma$:
\begin{equation}
\KL(q \| q + \dd q) \approx \frac{1}{2}\dd\mu^\T\Sigma^{-1}\dd\mu + \frac{1}{4}\tr(\Sigma^{-1}\dd\Sigma\Sigma^{-1}\dd\Sigma) = \frac{1}{2}\dd\tau^2
\end{equation}

This confirms that proper time $\dd\tau$ measures infinitesimal KL divergence.

\end{document}
