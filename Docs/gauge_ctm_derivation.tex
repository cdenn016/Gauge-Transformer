\documentclass[11pt,a4paper]{article}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{bbm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{xcolor}

\geometry{margin=1in}

% ============================================================================
% THEOREM ENVIRONMENTS
% ============================================================================
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{axiom}{Axiom}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% ============================================================================
% CUSTOM COMMANDS
% ============================================================================
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\pp}{\partial}
\newcommand{\T}{\mathsf{T}}

% ============================================================================
% TITLE
% ============================================================================
\title{%
    \textbf{From Information Geometry to Continuous Thought Machines:}\\[0.5em]
    \large A Rigorous Derivation from First Principles\\[0.3em]
    \normalsize Extended Version with Generative Model Foundation
}

\author{
    Gauge-Transformer Project\\
    \texttt{github.com/cdenn016/Gauge-Transformer}
}

\date{January 2026}

% ============================================================================
% DOCUMENT
% ============================================================================
\begin{document}

\maketitle

\begin{abstract}
We establish a rigorous mathematical connection between information-geometric gauge theory and the Continuous Thought Machine (CTM) architecture. Starting from a normalized generative model with auxiliary agreement variables, we derive the gauge-covariant variational free energy whose structure uniquely requires the forward KL divergence. We show that CTM's key innovations---neuron-level models, synchronization representations, and adaptive computation---emerge naturally from this framework. The derivation extends beyond Gaussians to arbitrary exponential families via the conditional uniqueness theorem: the forward KL is the \emph{only} divergence preserving exponential-family closure under variational optimization. We clarify the distinction between the Fisher-Rao metric (intrinsic geometry) and the free energy Hessian (dynamical mass), showing that proper time measures accumulated information change while cognitive inertia incorporates all sources of constraint.
\end{abstract}

\tableofcontents
\newpage

% ============================================================================
\section{Introduction}
% ============================================================================

The Continuous Thought Machine (CTM) \cite{darlow2025ctm} represents a significant departure from standard transformer architectures by introducing:
\begin{enumerate}[label=(\roman*)}
    \item \textbf{Neuron-level models (NLMs)}: Each neuron has private weights processing its activation history
    \item \textbf{Synchronization representations}: Pairwise temporal correlations as the latent space
    \item \textbf{Adaptive computation}: Internal ``ticks'' decoupled from input, halting based on confidence
\end{enumerate}

Independently, gauge-theoretic approaches to neural computation propose that:
\begin{enumerate}[label=(\roman*)]
    \item Beliefs are points on a \textbf{statistical manifold} with the Fisher-Rao metric
    \item \textbf{Proper time} is the information-theoretic arc length: ``a difference which makes a difference''
    \item Inter-agent coupling uses \textbf{KL divergence} with gauge-covariant transport
    \item Dynamics minimize a \textbf{variational free energy} derived from a normalized generative model
\end{enumerate}

This paper establishes that these frameworks are deeply connected: CTM's architecture can be \emph{derived} from information-geometric first principles. Unlike heuristic mappings, we ground the derivation in:
\begin{itemize}
    \item A \textbf{normalized generative model} with auxiliary agreement variables (Section~\ref{sec:generative_model})
    \item The \textbf{conditional uniqueness theorem} showing forward KL is the only consistent divergence (Section~\ref{sec:uniqueness})
    \item \textbf{Extension to exponential families} beyond Gaussians (Section~\ref{sec:exponential_families})
    \item Precise distinction between \textbf{Fisher metric and dynamical mass} (Section~\ref{sec:mass_matrix})
\end{itemize}

% ============================================================================
\section{The Normalized Generative Model}
\label{sec:generative_model}
% ============================================================================

We derive the variational free energy from first principles, showing that belief alignment emerges naturally from a normalized generative prior with auxiliary agreement variables. This construction justifies the KL-based coupling terms as consequences of gauge-transported Gaussian consistency constraints.

\subsection{Setup: Dual Latent Variables and Fiber Geometry}

Each agent $i$ maintains two distinct latent variables living in separate fiber bundles:
\begin{equation}
k_i \in \R^{d_q} \quad \text{(belief latent in $\mathcal{E}_q$)},
\qquad
m_i \in \R^{d_p} \quad \text{(model latent in $\mathcal{E}_p$)}.
\end{equation}

The full state of agent $i$ is a Gaussian distribution over each latent:
\begin{equation}
\begin{aligned}
q_i(k_i) &= \N(k_i\,;\mu_{q,i},\,\Sigma_{q,i}), \\
s_i(m_i) &= \N(m_i\,;\mu_{p,i},\,\Sigma_{p,i}).
\end{aligned}
\end{equation}

The fiber at each agent's location is the product statistical manifold:
\begin{equation}
\B(c) = \B_q \times \B_p,
\end{equation}
where $\B_q$ and $\B_p$ are spaces of Gaussian distributions (positive-definite mean-covariance pairs).

\subsection{Auxiliary Agreement Variables}

To enforce consistency between agents after gauge transport, we introduce auxiliary ``agreement'' variables for each ordered pair $(i,j)$:
\begin{equation}
z_{ij} \in \R^{d_q} \quad \text{(belief agreement)},
\qquad
w_{ij} \in \R^{d_p} \quad \text{(model agreement)}.
\end{equation}

These have clear interpretations:
\begin{itemize}
  \item $z_{ij}$: ``What agent $i$ believes agent $j$'s belief looks like, after transporting $j$'s belief into $i$'s gauge frame''
  \item $w_{ij}$: ``What agent $i$ believes agent $j$'s generative model looks like, after gauge transport''
\end{itemize}

Agreement variables are latent mediators that will be integrated out, leaving effective pairwise coupling between agents.

\subsection{The Normalized Joint Distribution}

The auxiliary variable $z_{ij}$ is drawn from the product of Gaussians:
\begin{equation}
p(z_{ij} \mid k_i, k_j) \propto
\N(z_{ij}; k_i, \Lambda_{ij}^{-1}) \cdot
\N(z_{ij}; \Omega_{ij}k_j, \Lambda_{ij}^{-1})
\end{equation}
where $\Omega_{ij} \in SO(N)$ is the gauge transport operator and $\Lambda_{ij}$ is the alignment precision.

The full joint generative model is:
\begin{equation}
\boxed{
\begin{aligned}
p&(\{k_i\},\{m_i\},\{z_{ij}\},\{w_{ij}\})\\
&=\left[\prod_i p_i(k_i)\,r_i(m_i)\right]\\
&\quad\times
\left[\prod_{i,j}
\N(z_{ij}\,;k_i,\Lambda_{ij}^{-1})
\,\N(z_{ij}\,;\Omega_{ij}k_j,\Lambda_{ij}^{-1})\right]\\
&\quad\times
\left[\prod_{i,j}
\N(w_{ij}\,;m_i,\Gamma_{ij}^{-1})
\,\N(w_{ij}\,;\tilde{\Omega}_{ij}m_j,\Gamma_{ij}^{-1})\right].
\end{aligned}
}
\end{equation}

\begin{theorem}[Normalization]
The joint distribution is properly normalized: $\int p(\cdot) = 1$.
\end{theorem}

\begin{proof}
Each factor is a normalized Gaussian. The products of Gaussians with identical precision yield normalized Gaussians after integrating over agreement variables. Since the joint is a product of normalized densities over disjoint variable sets, it is normalized. This is in contrast to unnormalized Markov random fields where the partition function $Z$ is intractable.
\end{proof}

\subsection{Derivation of the Variational Free Energy}

Under a mean-field posterior approximation:
\begin{equation}
q(\{k_i\},\{m_i\})=\prod_i q_i(k_i)\,s_i(m_i),
\end{equation}
the variational free energy becomes:
\begin{equation}
\F := \E_q[\log q] - \E_q[\log p] - \E_q[\log p(o|\cdot)]
\end{equation}

Expanding and integrating out auxiliary variables yields the quadratic forms:
\begin{equation}
\begin{aligned}
\F &= \sum_i D_{\KL}(q_i\,\|\,p_i) + \sum_i D_{\KL}(s_i\,\|\,r_i)\\
&\quad+\frac{1}{4}\sum_{i,j}\E_{q_i q_j}
\!\big[(k_i-\Omega_{ij}k_j)^\T\Lambda_{ij}(k_i-\Omega_{ij}k_j)\big]\\
&\quad+\frac{1}{4}\sum_{i,j}\E_{s_i s_j}
\!\big[(m_i-\tilde{\Omega}_{ij}m_j)^\T\Gamma_{ij}(m_i-\tilde{\Omega}_{ij}m_j)\big]\\
&\quad-\E_q[\log p(o\,|\{k_i\},\{m_i\})].
\end{aligned}
\end{equation}

\subsection{Relating Quadratic Forms to KL Divergences}

For the choice of precision:
\begin{equation}
\Lambda_{ij} := \tau^{(q)}_{ij}\,(\Omega_{ij}\Sigma_{q,j}\Omega_{ij}^\T)^{-1},
\end{equation}
the quadratic expectation becomes (in the alignment regime where $\Sigma_i \approx \Omega_{ij}\Sigma_j\Omega_{ij}^\T$):
\begin{equation}
\frac{1}{4}\E_{q_i q_j}[(k_i-\Omega_{ij}k_j)^\T\Lambda_{ij}(k_i-\Omega_{ij}k_j)]
\approx \frac{\tau^{(q)}_{ij}}{2}\,D_{\KL}(q_i\,\|\,\Omega_{ij}q_j)+\text{const}.
\end{equation}

Defining normalized weights $\beta_{ij} := \tau^{(q)}_{ij}/2$, we obtain:

\begin{tcolorbox}[colback=blue!5,colframe=blue!50,title=Final Form of Variational Free Energy]
\begin{equation}
\boxed{
\begin{aligned}
\F[\{q_i\},\{s_i\}]
&=\sum_i D_{\KL}(q_i\,\|\,p_i)
+\sum_i D_{\KL}(s_i\,\|\,r_i)\\
&\quad+\sum_{i,j}\beta_{ij}\,D_{\KL}(q_i\,\|\,\Omega_{ij}q_j)\\
&\quad+\sum_{i,j}\gamma_{ij}\,D_{\KL}(s_i\,\|\,\tilde{\Omega}_{ij}s_j)\\
&\quad-\E_q[\log p(o\,|\{k_i\},\{m_i\})].
\end{aligned}
}
\end{equation}
\end{tcolorbox}

% ============================================================================
\section{Conditional Uniqueness of the Forward KL Divergence}
\label{sec:uniqueness}
% ============================================================================

We now establish that the forward KL is not merely a modeling choice but a \emph{necessary consequence} of the variational structure.

\subsection{The Variational Game}

Each agent $i$ minimizes a local free-energy functional:
\begin{equation}
F_i[\beta_i] = \min_{q_i} \Big\{ D_{\KL}(q_i \,\|\, p_i) + \sum_{j\neq i} \beta_{ij}\,\mathcal{D}(q_i,q_j) \Big\},
\end{equation}
where $\mathcal{D}$ is some divergence to be determined.

\subsection{Forward KL Yields Geometric-Mean Solution}

For $\mathcal{D}(q_i,q_j) = D_{\KL}(q_i \,\|\, \Omega_{ij} q_j)$, the stationary condition yields:
\begin{equation}
q_i^*(c) = \frac{1}{Z_i} e^{-H_i(c)/2} \prod_j [\Omega_{ij}(c) q_j(c)]^{\beta_{ij}/2},
\end{equation}
where $H_i = -\log p_i$ is the local ``energy.'' This is a \textbf{geometric average} of transported neighbor beliefs---a closed-form Boltzmann distribution.

\subsection{Failure of Alternative Divergences}

\paragraph{Reverse KL.} If $\mathcal{D} = D_{\KL}(\Omega_{ij}q_j \,\|\, q_i)$, the stationary condition becomes:
\begin{equation}
H_i(c) + \log q_i(c) - \sum_j \beta_{ij} \frac{\Omega_{ij}(c) q_j(c)}{q_i(c)} = \text{const},
\end{equation}
introducing $1/q_i$ terms that destroy exponential-family closure.

\paragraph{Symmetric KL.} The symmetrized divergence mixes $\log q_i$ and $1/q_i$ terms, again breaking log-linearity.

\subsection{The Conditional Uniqueness Theorem}

\begin{theorem}[Conditional Uniqueness]
\label{thm:uniqueness}
Let $\mathcal{D}(q_i,q_j)$ be any local $f$-divergence
\[
\mathcal{D}(q_i,q_j) = \int q_i(c)\, f\!\left(\frac{q_i(c)}{\Omega_{ij}(c) q_j(c)}\right) dc,
\]
that enters linearly in the free energy, and suppose the stationary distribution $q_i^*$ is log-linear in $\{H_i,\Omega_{ij}q_j\}$.

Then the following are equivalent:
\begin{enumerate}
  \item $q_i^*$ has the geometric-mean Boltzmann form;
  \item $\mathcal{D}(q_i,q_j) = D_{\KL}(q_i \,\|\, \Omega_{ij} q_j)$;
  \item $C_{ij} = \dfrac{\partial F_i}{\partial\beta_{ij}} = D_{\KL}(q_i^* \,\|\, \Omega_{ij}q_j)$.
\end{enumerate}
\end{theorem}

\begin{proof}[Proof sketch]
$(2) \Rightarrow (1)$: Direct solution of stationary condition.

$(1) \Rightarrow (2)$: Substituting log-linear form into stationarity gives functional derivative matching forward KL. The requirement $\mathcal{D}(q,q)=0$ fixes the form uniquely.

$(3) \Rightarrow (2)$: By the envelope theorem, the only divergence consistent with linear $\beta_{ij}$-coupling and this derivative structure is the forward KL.
\end{proof}

\begin{tcolorbox}[colback=green!5,colframe=green!50,title=Key Result]
The forward KL divergence $D_{\KL}(q_i \| \Omega_{ij} q_j)$ is the \textbf{unique} divergence that:
\begin{enumerate}
    \item Preserves exponential-family closure
    \item Gives closed-form Gibbs solution
    \item Has consistent dual interpretation for attention weights
    \item Is gauge-covariant under $SO(N)$ or $SU(N)$
\end{enumerate}
\end{tcolorbox}

% ============================================================================
\section{Extension to Exponential Families}
\label{sec:exponential_families}
% ============================================================================

The conditional uniqueness theorem immediately generalizes beyond Gaussians.

\subsection{Exponential Family Distributions}

An exponential family has the form:
\begin{equation}
p(x; \theta) = h(x) \exp\left(\theta^\T T(x) - A(\theta)\right),
\end{equation}
where $\theta$ are natural parameters, $T(x)$ are sufficient statistics, and $A(\theta)$ is the log-partition function.

\begin{example}[Common Exponential Families]
\begin{itemize}
    \item \textbf{Gaussian}: $\theta = (\Sigma^{-1}\mu, -\frac{1}{2}\Sigma^{-1})$, $T(x) = (x, xx^\T)$
    \item \textbf{Categorical}: $\theta = \log(\pi_k/\pi_K)$, $T(x) = \mathbf{1}_{x=k}$ (one-hot)
    \item \textbf{Poisson}: $\theta = \log\lambda$, $T(x) = x$
    \item \textbf{Von Mises-Fisher}: $\theta = \kappa\mu$, $T(x) = x$ (on sphere $S^{d-1}$)
    \item \textbf{Wishart}: For covariance matrices
\end{itemize}
\end{example}

\subsection{Fisher-Rao Metric for Exponential Families}

The Fisher information matrix for an exponential family is:
\begin{equation}
G(\theta) = \nabla^2 A(\theta) = \text{Cov}_{p_\theta}[T(X)].
\end{equation}

This is the Hessian of the log-partition function, which equals the covariance of sufficient statistics.

\subsection{KL Divergence as Bregman Divergence}

For exponential families, the KL divergence is the Bregman divergence generated by $A(\theta)$:
\begin{equation}
D_{\KL}(p_{\theta_1} \| p_{\theta_2}) = A(\theta_2) - A(\theta_1) - \nabla A(\theta_1)^\T(\theta_2 - \theta_1).
\end{equation}

\begin{theorem}[Exponential Family Generalization]
The variational free energy structure and conditional uniqueness theorem extend to any exponential family. Specifically:
\begin{enumerate}
    \item The free energy becomes $\F = \sum_i D_{\KL}(q_i \| p_i) + \sum_{ij} \beta_{ij} D_{\KL}(q_i \| \Omega_{ij} q_j) + \ldots$
    \item The stationary solution has geometric-mean form in natural parameters
    \item The Fisher-Rao metric is $G(\theta) = \nabla^2 A(\theta)$
    \item Proper time is $d\tau^2 = d\theta^\T G(\theta) d\theta$
\end{enumerate}
\end{theorem}

\subsection{Implications for CTM}

This generalization means CTM-like architectures can be constructed for:
\begin{itemize}
    \item \textbf{Categorical distributions}: Attention over discrete tokens (standard transformers!)
    \item \textbf{Mixture models}: For multi-modal beliefs
    \item \textbf{Directional data}: Von Mises-Fisher for embeddings on spheres
    \item \textbf{Count data}: Poisson for event-based neural coding
\end{itemize}

% ============================================================================
\section{Proper Time as Information-Theoretic Arc Length}
\label{sec:proper_time}
% ============================================================================

A fundamental issue in any dynamical theory of belief is the definition of time. Wall-clock time is unsuitable because cognitive processes operate on different timescales for different agents and contexts.

\subsection{Definition}

We define proper time as the information-theoretic arc length on the statistical manifold:

\begin{definition}[Proper Time]
For an infinitesimal belief change $d\mu$, the proper time increment is:
\begin{equation}
\boxed{d\tau = \sqrt{d\mu^\T \Sigma^{-1} d\mu} = \|d\mu\|_{\Sigma^{-1}}}
\end{equation}
\end{definition}

\subsection{Properties}

\paragraph{Information-theoretic interpretation.} To second order, both KL directions give the same result:
\begin{equation}
\KL(q + dq \| q) \approx \KL(q \| q + dq) \approx \frac{1}{2} d\mu^\T \Sigma^{-1} d\mu = \frac{1}{2} d\tau^2
\end{equation}
Thus proper time measures accumulated information change.

\paragraph{Scale dependence.} For a high-precision agent ($\Sigma$ small, $\Sigma^{-1}$ large), a small change in $\mu$ corresponds to large proper time---``time moves fast'' in the sense that each update is cognitively significant. For a low-precision agent, the same parametric change corresponds to small proper time.

\paragraph{Invariance.} Proper time is invariant under reparameterization of the belief space, depending only on the intrinsic geometry of the statistical manifold.

\subsection{Generalization to Exponential Families}

For a general exponential family with natural parameters $\theta$:
\begin{equation}
d\tau^2 = d\theta^\T G(\theta) d\theta = d\theta^\T \nabla^2 A(\theta) d\theta
\end{equation}

This is the arc length with respect to the Fisher-Rao metric on the statistical manifold.

% ============================================================================
\section{Fisher Metric versus Dynamical Mass}
\label{sec:mass_matrix}
% ============================================================================

A crucial distinction must be made between the \textbf{intrinsic geometry} of the statistical manifold and the \textbf{dynamical mass} governing belief evolution.

\subsection{The Fisher-Rao Metric (Intrinsic Geometry)}

The Fisher-Rao metric is the natural Riemannian metric on the statistical manifold:
\begin{equation}
G_{\mu\nu}(\theta) = \E_{q_\theta}\left[\frac{\pp \log q_\theta}{\pp \theta^\mu} \cdot \frac{\pp \log q_\theta}{\pp \theta^\nu}\right]
\end{equation}

For Gaussians: $G_{\mu\mu} = \Sigma^{-1}$ (belief precision).

This metric determines:
\begin{itemize}
    \item Proper time (arc length)
    \item Natural gradients
    \item Geodesics on the manifold
\end{itemize}

\subsection{The Mass Matrix (Dynamical Inertia)}

The effective mass for Hamiltonian belief dynamics is the \textbf{Hessian of the free energy}:
\begin{equation}
\mathbf{M} = \frac{\partial^2 \F}{\partial\xi\partial\xi^\T}
\end{equation}
where $\xi = (\mu_1, \ldots, \mu_N, \Sigma_1, \ldots, \Sigma_N)$ is the full state vector.

\subsection{Contributions to Dynamical Mass}

The mass matrix receives contributions from \textbf{all sources of constraint}:

\paragraph{1. Prior Precision.}
\begin{equation}
\frac{\partial^2}{\partial\mu_i\partial\mu_i^\T} D_{\KL}(q_i \| p_i) = \Sigma_{p,i}^{-1}
\end{equation}
Resistance to deviating from prior expectations.

\paragraph{2. Observation Precision.}
For likelihood $p(o|k) = \N(o; Hk, R)$:
\begin{equation}
\frac{\partial^2}{\partial\mu_i\partial\mu_i^\T} \left(-\E_{q_i}[\log p(o|k)]\right) = H^\T R^{-1} H
\end{equation}
Fisher information of the observation model.

\paragraph{3. Social Coupling Precision.}
\begin{equation}
\frac{\partial^2}{\partial\mu_i\partial\mu_i^\T} \sum_k \beta_{ik} D_{\KL}(q_i \| \Omega_{ik} q_k) = \sum_k \beta_{ik} (\Omega_{ik}\Sigma_k\Omega_{ik}^\T)^{-1}
\end{equation}
Weighted precision from all neighbors.

\paragraph{Total Mass.}
\begin{equation}
\boxed{
\mathbf{M}_i = \Sigma_{p,i}^{-1} + H^\T R^{-1} H + \sum_k \beta_{ik} (\Omega_{ik}\Sigma_k\Omega_{ik}^\T)^{-1}
}
\end{equation}

\begin{tcolorbox}[colback=yellow!5,colframe=yellow!50,title=Key Distinction]
\begin{itemize}
    \item \textbf{Fisher-Rao metric} $G = \Sigma^{-1}$: Intrinsic geometry of belief space
    \item \textbf{Mass matrix} $\mathbf{M} = \partial^2\F/\partial\xi^2$: Total resistance to belief change from all constraints
\end{itemize}
The mass matrix \emph{includes} the Fisher metric but also incorporates prior, observation, and social terms.
\end{tcolorbox}

\subsection{Implications for CTM}

In CTM, each neuron's ``inertia'' (resistance to activation change) should be understood as the local Hessian of the effective loss function, not just the intrinsic geometry of the activation space. This explains why neurons with strong priors or tight coupling to neighbors are more stable (higher effective mass).

% ============================================================================
\section{Derivation of CTM Architecture}
\label{sec:ctm_derivation}
% ============================================================================

We now derive each component of CTM from the information-geometric framework.

\subsection{Neuron-Level Models from Local Metrics}

\begin{theorem}[NLM Structure]
If each computational unit $i$ maintains a distribution $q_i$ on a statistical manifold with unit-specific parameters, then optimal parameterization requires unit-specific weight matrices $W_i$ that determine the local metric:
\begin{equation}
G_i^{(x)} = W_i^\T \Sigma_i^{-1} W_i
\end{equation}
where the input-space Fisher metric is obtained by chain rule from the belief-space Fisher metric.
\end{theorem}

\begin{proof}
Consider unit $i$ mapping inputs $x \in \R^n$ to a belief $q_i(z; x) = \N(W_i x, \Sigma_i)$.

By the Fisher information chain rule:
\begin{equation}
G_i^{(x)} = \frac{\pp \mu}{\pp x}^\T G_i^{(\mu)} \frac{\pp \mu}{\pp x} = W_i^\T \Sigma_i^{-1} W_i
\end{equation}

For unit-specific sensitivity to inputs, we require unit-specific $W_i$---precisely the NLM structure where each neuron has private weights processing its input history.
\end{proof}

\begin{corollary}[Precision Dynamics]
High-precision units (large $\Sigma_i^{-1}$) experience faster proper time:
\begin{equation}
d\tau_i = \|dx\|_{G_i} = \sqrt{dx^\T W_i^\T \Sigma_i^{-1} W_i\, dx}
\end{equation}
\end{corollary}

\subsection{Synchronization from KL Alignment}

\begin{lemma}[Mutual Information as KL Divergence]
\begin{equation}
I(X; Y) = D_{\KL}(p_{XY} \| p_X \otimes p_Y)
\end{equation}
\end{lemma}

\begin{lemma}[Correlation and Mutual Information for Gaussians]
For jointly Gaussian $(X, Y)$ with correlation $\rho$:
\begin{equation}
I(X; Y) = -\frac{1}{2}\log(1 - \rho^2)
\end{equation}
\end{lemma}

\begin{theorem}[Synchronization as Information-Geometric Alignment]
\label{thm:sync}
CTM's synchronization matrix $S_{ij} = \text{corr}(x_i, x_j)$ is related to KL-based alignment through mutual information:
\begin{equation}
|S_{ij}| \approx \sqrt{2 I(x_i; x_j)} = \sqrt{2 D_{\KL}(p_{ij} \| p_i \otimes p_j)}
\end{equation}
for small correlations.
\end{theorem}

\begin{proof}
From Lemma 2: $I(X;Y) = -\frac{1}{2}\log(1 - \rho^2)$.

Inverting: $\rho^2 = 1 - e^{-2I}$.

For small $I$: $\rho^2 \approx 2I$, hence $|\rho| \approx \sqrt{2I} = \sqrt{2 D_{\KL}(p_{ij} \| p_i p_j)}$.

Both frameworks implement: \textbf{attention weight = softmax of (negative) information distance}.
\end{proof}

\subsection{Adaptive Computation from Proper Time}

\begin{theorem}[Adaptive Halting]
If computation halts when proper time rate falls below threshold:
\begin{equation}
\frac{d\tau}{dt} < \epsilon \quad \text{for } k \text{ consecutive ticks}
\end{equation}
then:
\begin{enumerate}
    \item Simple inputs (rapid convergence) $\Rightarrow$ early halt
    \item Complex inputs (prolonged evolution) $\Rightarrow$ late halt
    \item High-precision units process faster (more $\tau$ per tick)
\end{enumerate}
\end{theorem}

\begin{proof}
Simple inputs: Belief stabilizes quickly $\Rightarrow$ $d\mu_t \to 0$ $\Rightarrow$ $d\tau/dt \to 0$ early.

Complex inputs: Belief continues evolving $\Rightarrow$ $d\tau/dt$ remains above threshold.

Precision: For fixed $|d\mu|$, proper time scales as $d\tau \propto \sqrt{\lambda_{\max}(\Sigma^{-1})}$.
\end{proof}

\begin{remark}[Connection to CTM Halting]
CTM halts based on ``tick with lowest loss + highest certainty.'' In information geometry:
\begin{equation}
\text{Certainty} \propto |\Sigma^{-1}| = \text{precision}
\end{equation}
When precision is high and stable, $d\tau$ becomes small---the system is confident and unchanging. This matches the CTM halting condition.
\end{remark}

% ============================================================================
\section{Summary of Correspondences}
\label{sec:summary}
% ============================================================================

\begin{table}[h]
\centering
\caption{Correspondence between Gauge Theory and CTM}
\label{tab:correspondence}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Gauge Theory} & \textbf{CTM} & \textbf{Derivation} \\
\midrule
Statistical manifold $\M$ & State space of neurons & By construction \\
Generative model with $z_{ij}$ & Implicit in architecture & Section~\ref{sec:generative_model} \\
Forward KL (unique) & Correlation-based sync & Theorem~\ref{thm:uniqueness} \\
Fisher metric $G = \Sigma^{-1}$ & NLM input sensitivity & Section~\ref{sec:ctm_derivation} \\
Mass matrix $\mathbf{M} = \partial^2\F$ & Full inertia (priors + obs + social) & Section~\ref{sec:mass_matrix} \\
Proper time $d\tau = \|d\mu\|_{\Sigma^{-1}}$ & Adaptive iteration & Theorem 5 \\
$\beta_{ij} = \softmax(-\KL/\kappa)$ & Attention from sync & Theorem~\ref{thm:sync} \\
Exponential family extension & Categorical, Poisson, etc. & Section~\ref{sec:exponential_families} \\
Gauge covariance $\Omega_{ij}$ & Correlation invariance & Implicit \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
\section{Gaps and Required Assumptions}
\label{sec:gaps}
% ============================================================================

\subsection{Gap 1: Gaussian vs. General Exponential Family}

\textbf{Status:} Fully bridged by the conditional uniqueness theorem. The derivation extends to any exponential family.

\subsection{Gap 2: Temporal vs. Instantaneous}

\textbf{Issue:} Gauge theory is formulated for instantaneous beliefs; CTM uses histories.

\textbf{Bridge:} Define temporal distributions over trajectories. The Fisher metric on trajectory space averages instantaneous metrics:
\begin{equation}
G^{\text{temporal}} = \frac{1}{T} \sum_{t=1}^T G(\theta(t))
\end{equation}

\subsection{Gap 3: Synchronization $\neq$ KL (Exactly)}

\textbf{Status:} Connected via mutual information (Theorem~\ref{thm:sync}). The relationship is exact for Gaussians and approximate for general distributions.

\subsection{Gap 4: Gauge Transport}

\textbf{Issue:} CTM has no explicit gauge structure.

\textbf{Resolution:} Correlation is already gauge-invariant:
\begin{itemize}
    \item Shifts: $x \mapsto x + c$ (correlation unchanged)
    \item Scalings: $x \mapsto \alpha x$ (correlation unchanged)
    \item Rotations: $x \mapsto Rx$ (correlation magnitude preserved)
\end{itemize}
CTM implicitly has gauge covariance.

\subsection{Gap 5: Explicit vs. Implicit Optimization}

\textbf{Issue:} Gauge theory explicitly minimizes $\F$; CTM uses synchronization as representation.

\textbf{Resolution:} Gradients through prediction pathway create implicit alignment pressure:
\begin{equation}
\frac{\pp \mathcal{L}}{\pp W_d} = \frac{\pp \mathcal{L}}{\pp \hat{y}} \cdot \frac{\pp \hat{y}}{\pp S} \cdot \frac{\pp S}{\pp a} \cdot \frac{\pp a}{\pp W_d}
\end{equation}
Functionally equivalent to explicit free energy optimization.

% ============================================================================
\section{Proposed Architecture: Gauge-Theoretic CTM}
\label{sec:proposal}
% ============================================================================

Based on the derivations, we propose explicit modifications to CTM.

\subsection{Replace Correlation with KL-Based Synchronization}

Instead of $S_{ij} = \text{corr}(x_i, x_j)$, use:
\begin{equation}
S_{ij}^{\text{gauge}} = \exp\left(-\frac{D_{\KL}(q_i \| \Omega_{ij}[q_j])}{\kappa}\right)
\end{equation}
where $q_i = \N(\bar{x}_i, \hat{\Sigma}_i)$ is estimated from activation history.

\subsection{Replace Internal Ticks with Proper Time}

Iterate until:
\begin{equation}
\Delta\tau = \sum_{d} \sqrt{\Delta\mu_d^\T \Sigma_d^{-1} \Delta\mu_d} < \epsilon
\end{equation}

\subsection{Add Explicit Gauge Transport}

Include learnable transport operators:
\begin{equation}
\Omega_{ij} = \exp\left(\sum_a \omega_{ij}^a T_a\right) \in SO(K)
\end{equation}

\subsection{Use Variational Free Energy as Loss}

\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{task}} + \alpha \sum_i D_{\KL}(q_i \| p_i) + \lambda \sum_{i,j} \beta_{ij} D_{\KL}(q_i \| \Omega_{ij}[q_j])
\end{equation}

% ============================================================================
\section{Conclusion}
\label{sec:conclusion}
% ============================================================================

We have established that CTM architecture can be rigorously derived from information-geometric first principles:

\begin{enumerate}
    \item \textbf{The generative model} with auxiliary agreement variables yields the gauge-covariant free energy (Section~\ref{sec:generative_model})

    \item \textbf{The forward KL is unique}: The conditional uniqueness theorem proves it is the only divergence preserving exponential-family closure (Section~\ref{sec:uniqueness})

    \item \textbf{Extension to exponential families}: The entire framework generalizes beyond Gaussians (Section~\ref{sec:exponential_families})

    \item \textbf{Fisher metric vs. mass matrix}: Proper time uses the intrinsic metric; dynamical inertia uses the free energy Hessian (Section~\ref{sec:mass_matrix})

    \item \textbf{CTM components derived}: NLMs from local metrics, synchronization from KL alignment, adaptive compute from proper time (Section~\ref{sec:ctm_derivation})
\end{enumerate}

The key mathematical identities bridging the frameworks:
\begin{align}
I(X;Y) &= D_{\KL}(p_{XY} \| p_X p_Y) \\
D_{\KL}(q \| q + dq) &\approx \frac{1}{2} d\tau^2 \\
\mathbf{M} &= \Sigma_p^{-1} + H^\T R^{-1} H + \sum_k \beta_{ik} \Sigma_k^{-1}
\end{align}

This unification suggests that biological neural synchronization, gauge-theoretic belief dynamics, and modern neural architectures share deep mathematical structure rooted in information geometry and the calculus of variations.

% ============================================================================
% REFERENCES
% ============================================================================
\begin{thebibliography}{99}

\bibitem{darlow2025ctm}
L.~Darlow, C.~Regan, S.~Risi, J.~Seely, and L.~Jones.
\newblock Continuous Thought Machines.
\newblock \emph{arXiv preprint arXiv:2505.05522}, 2025.

\bibitem{amari1998natural}
S.~Amari.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural Computation}, 10(2):251--276, 1998.

\bibitem{friston2010free}
K.~Friston.
\newblock The free-energy principle: a unified brain theory?
\newblock \emph{Nature Reviews Neuroscience}, 11(2):127--138, 2010.

\bibitem{ay2017information}
N.~Ay, J.~Jost, H.~V.~L\^e, and L.~Schwachh\"ofer.
\newblock \emph{Information Geometry}.
\newblock Springer, 2017.

\bibitem{tishby2000information}
N.~Tishby, F.~C.~Pereira, and W.~Bialek.
\newblock The information bottleneck method.
\newblock \emph{arXiv preprint physics/0004057}, 2000.

\bibitem{nakahara2003geometry}
M.~Nakahara.
\newblock \emph{Geometry, Topology and Physics}.
\newblock CRC Press, 2003.

\bibitem{amari2016information}
S.~Amari.
\newblock \emph{Information Geometry and Its Applications}.
\newblock Springer, 2016.

\end{thebibliography}

\end{document}
