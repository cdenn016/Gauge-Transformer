"""
Pure Free Energy Principle Transformer - Built from First Principles

This implements a transformer where ALL dynamics emerge from minimizing a single
Variational Free Energy functional. No ad hoc neural network components.

Mathematical Foundation:
========================
The VFE functional over beliefs q_i = N(Î¼_i, Î£_i) with gauge frames Ï†_i:

F[q] = Î£_i F_self[q_i]                           # Self-coupling (uncertainty cost)
     + Î£_{i,j} F_align[q_i, q_j; Î©_ij]           # Belief alignment (attention)
     + Î£_i F_prior[q_i, Ï€_i]                     # Prior coupling (memory)
     + Î£_i F_obs[q_i, y_i]                       # Observation (prediction)

where:
- Î©_ij = exp(Ï†_ij) is the gauge transport from frame j to frame i
- Ï†_ij = BCH(Ï†_i, -Ï†_j) = Ï†_i - Ï†_j - Â½[Ï†_i, Ï†_j]_ð”¤ + O(Ï†Â³)
- [Â·,Â·]_ð”¤ is the Lie bracket with structure constants f_abc

Key Design Choices:
==================
1. SO(N) gauge group with fundamental representation
2. BCH formula for transport (not naive subtraction)
3. Block-diagonal covariance aligned with irreps
4. Haar initialization for symmetry breaking
5. Natural gradient descent respecting Fisher-Rao geometry

Author: Built from scratch following the FEP papers
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple, List
from dataclasses import dataclass


# =============================================================================
# PART 1: SO(N) LIE ALGEBRA
# =============================================================================

def so_n_generators(n: int, device='cpu', dtype=torch.float32) -> torch.Tensor:
    """
    Generate the basis for so(n) Lie algebra (antisymmetric matrices).

    The Lie algebra so(n) has dimension n(n-1)/2.
    Basis elements T_{ab} (a < b) have:
        (T_{ab})_{ij} = Î´_{ai}Î´_{bj} - Î´_{aj}Î´_{bi}

    Args:
        n: Dimension of SO(N)

    Returns:
        generators: (n_gen, n, n) tensor where n_gen = n(n-1)/2
    """
    n_gen = n * (n - 1) // 2
    generators = torch.zeros(n_gen, n, n, device=device, dtype=dtype)

    idx = 0
    for a in range(n):
        for b in range(a + 1, n):
            generators[idx, a, b] = 1.0
            generators[idx, b, a] = -1.0
            idx += 1

    return generators


def so_n_structure_constants(n: int, device='cpu', dtype=torch.float32) -> torch.Tensor:
    """
    Compute structure constants f_abc for so(n).

    The Lie bracket is: [T_a, T_b] = Î£_c f_abc T_c

    For so(n), the structure constants come from:
        [T_{ij}, T_{kl}] = Î´_{jk}T_{il} - Î´_{ik}T_{jl} - Î´_{jl}T_{ik} + Î´_{il}T_{jk}

    Args:
        n: Dimension of SO(N)

    Returns:
        f_abc: (n_gen, n_gen, n_gen) structure constants
    """
    generators = so_n_generators(n, device, dtype)
    n_gen = generators.shape[0]

    # Compute [T_a, T_b] for all pairs
    # [A, B] = AB - BA
    commutators = torch.einsum('aij,bjk->abik', generators, generators) - \
                  torch.einsum('bij,ajk->abik', generators, generators)

    # Project onto basis to get f_abc
    # [T_a, T_b] = f_abc T_c  =>  f_abc = Tr([T_a, T_b] T_c^T) / Tr(T_c T_c^T)
    # For orthonormal basis: f_abc = -Â½ Tr([T_a, T_b] T_c)
    # Our generators have Tr(T_a T_b^T) = 2 Î´_{ab}, so normalize

    f_abc = torch.einsum('abij,cji->abc', commutators, generators) / 2.0

    return f_abc


def lie_bracket(phi_1: torch.Tensor, phi_2: torch.Tensor,
                f_abc: torch.Tensor) -> torch.Tensor:
    """
    Compute Lie bracket [Ï†_1, Ï†_2]_ð”¤ using structure constants.

    If Ï†_1 = Î±^a T_a and Ï†_2 = Î²^b T_b, then:
        [Ï†_1, Ï†_2] = Î±^a Î²^b [T_a, T_b] = Î±^a Î²^b f_abc T_c

    In coefficient form: [Ï†_1, Ï†_2]^c = f_abc Î±^a Î²^b

    Args:
        phi_1: (..., dim_g) Lie algebra coefficients
        phi_2: (..., dim_g) Lie algebra coefficients
        f_abc: (dim_g, dim_g, dim_g) structure constants

    Returns:
        bracket: (..., dim_g) coefficients of [Ï†_1, Ï†_2]
    """
    # [Ï†_1, Ï†_2]^c = f_abc Ï†_1^a Ï†_2^b
    return torch.einsum('abc,...a,...b->...c', f_abc, phi_1, phi_2)


# =============================================================================
# PART 2: BAKER-CAMPBELL-HAUSDORFF TRANSPORT
# =============================================================================

def bch_combine(phi_i: torch.Tensor, phi_j: torch.Tensor,
                f_abc: torch.Tensor, order: int = 2) -> torch.Tensor:
    """
    Combine gauge frames using Baker-Campbell-Hausdorff formula.

    BCH formula: log(exp(X)exp(Y)) = X + Y + Â½[X,Y] + 1/12[X,[X,Y]] - 1/12[Y,[X,Y]] + ...

    For transport from frame j to frame i: Ï†_ij = BCH(Ï†_i, -Ï†_j)

    Args:
        phi_i: (..., dim_g) source frame coefficients
        phi_j: (..., dim_g) target frame coefficients
        f_abc: (dim_g, dim_g, dim_g) structure constants
        order: BCH truncation order (1=sum, 2=+commutator, 3=+nested)

    Returns:
        phi_ij: (..., dim_g) transport coefficients
    """
    # Order 1: Just sum (naive, but sometimes sufficient for small Ï†)
    phi_ij = phi_i - phi_j

    if order >= 2:
        # Order 2: Add first commutator term
        # BCH: X + Y + Â½[X, Y] where X=Ï†_i, Y=-Ï†_j
        # = Ï†_i - Ï†_j + Â½[Ï†_i, -Ï†_j] = Ï†_i - Ï†_j - Â½[Ï†_i, Ï†_j]
        bracket = lie_bracket(phi_i, phi_j, f_abc)
        phi_ij = phi_ij - 0.5 * bracket

    if order >= 3:
        # Order 3: Add nested commutator terms
        # + 1/12[X,[X,Y]] - 1/12[Y,[X,Y]]
        # = 1/12[Ï†_i, [Ï†_i, -Ï†_j]] - 1/12[-Ï†_j, [Ï†_i, -Ï†_j]]
        # = -1/12[Ï†_i, [Ï†_i, Ï†_j]] - 1/12[Ï†_j, [Ï†_i, Ï†_j]]
        bracket_ij = lie_bracket(phi_i, phi_j, f_abc)
        nested_i = lie_bracket(phi_i, bracket_ij, f_abc)
        nested_j = lie_bracket(phi_j, bracket_ij, f_abc)
        phi_ij = phi_ij - (1/12) * (nested_i + nested_j)

    return phi_ij


def exp_so_n(phi: torch.Tensor, generators: torch.Tensor,
             max_terms: int = 6) -> torch.Tensor:
    """
    Compute exp(Ï†) for Ï† in so(n) using matrix exponential via series.

    exp(A) = I + A + AÂ²/2! + AÂ³/3! + ...

    For small ||Ï†||, truncated series is accurate and efficient.

    Args:
        phi: (..., dim_g) Lie algebra coefficients
        generators: (dim_g, n, n) basis matrices
        max_terms: Number of series terms

    Returns:
        R: (..., n, n) rotation matrices in SO(N)
    """
    # Construct matrix A = Ï†^a T_a
    # phi: (..., dim_g), generators: (dim_g, n, n)
    A = torch.einsum('...a,aij->...ij', phi, generators)

    n = generators.shape[1]
    batch_shape = phi.shape[:-1]

    # Initialize: exp(A) = I
    I = torch.eye(n, device=phi.device, dtype=phi.dtype)
    I = I.expand(*batch_shape, n, n)

    result = I.clone()
    A_power = I.clone()  # A^0 = I

    for k in range(1, max_terms):
        A_power = torch.einsum('...ij,...jk->...ik', A_power, A) / k
        result = result + A_power

    return result


def rodrigues_so3(phi: torch.Tensor) -> torch.Tensor:
    """
    Rodrigues formula for SO(3) - efficient closed form.

    exp(Ï†) = I + (sin Î¸ / Î¸) A + ((1 - cos Î¸) / Î¸Â²) AÂ²

    where Î¸ = ||Ï†|| and A is the Lie algebra element (skew-symmetric matrix).

    IMPORTANT: Uses the same generator basis as so_n_generators(3):
        T_0: (0,1)=1, (1,0)=-1  (xy-plane rotation)
        T_1: (0,2)=1, (2,0)=-1  (xz-plane rotation)
        T_2: (1,2)=1, (2,1)=-1  (yz-plane rotation)

    So A = Ï†_0 T_0 + Ï†_1 T_1 + Ï†_2 T_2 gives:
        A = [[0, Ï†_0, Ï†_1], [-Ï†_0, 0, Ï†_2], [-Ï†_1, -Ï†_2, 0]]

    Only valid for SO(3) (dim_g = 3)!

    Args:
        phi: (..., 3) Lie algebra coefficients

    Returns:
        R: (..., 3, 3) rotation matrix
    """
    theta = torch.norm(phi, dim=-1, keepdim=True).unsqueeze(-1)  # (..., 1, 1)
    theta = theta.clamp(min=1e-8)  # Avoid division by zero

    # Skew-symmetric matrix A = Ï†^a T_a matching so_n_generators(3) basis
    # A = [[0, Ï†_0, Ï†_1], [-Ï†_0, 0, Ï†_2], [-Ï†_1, -Ï†_2, 0]]
    batch_shape = phi.shape[:-1]
    A = torch.zeros(*batch_shape, 3, 3, device=phi.device, dtype=phi.dtype)
    A[..., 0, 1] = phi[..., 0]
    A[..., 0, 2] = phi[..., 1]
    A[..., 1, 0] = -phi[..., 0]
    A[..., 1, 2] = phi[..., 2]
    A[..., 2, 0] = -phi[..., 1]
    A[..., 2, 1] = -phi[..., 2]

    I = torch.eye(3, device=phi.device, dtype=phi.dtype).expand(*batch_shape, 3, 3)

    # Rodrigues formula
    sin_theta = torch.sin(theta)
    cos_theta = torch.cos(theta)

    R = I + (sin_theta / theta) * A + \
        ((1 - cos_theta) / (theta ** 2)) * torch.einsum('...ij,...jk->...ik', A, A)

    return R


# =============================================================================
# PART 3: GAUSSIAN BELIEFS WITH BLOCK-DIAGONAL COVARIANCE
# =============================================================================

class IrrepSpec:
    """
    Specification of irreducible representations.

    Example: [('fund', 3, 10)] means 3 copies of 10-dim fundamental rep
    Total dim = 3 * 10 = 30
    """
    def __init__(self, specs: List[Tuple[str, int, int]]):
        """
        Args:
            specs: List of (name, multiplicity, dimension) tuples
        """
        self.specs = specs
        self.block_dims = []
        self.block_starts = []

        offset = 0
        for name, mult, dim in specs:
            for _ in range(mult):
                self.block_starts.append(offset)
                self.block_dims.append(dim)
                offset += dim

        self.total_dim = offset
        self.n_blocks = len(self.block_dims)

    def __repr__(self):
        return f"IrrepSpec({self.specs}, total_dim={self.total_dim})"


class BlockDiagonalCovariance:
    """
    Block-diagonal covariance matrix aligned with irrep structure.

    Stores r blocks of sizes (d_1, d_1), (d_2, d_2), ..., (d_r, d_r)
    Memory: O(Î£ d_iÂ²) instead of O((Î£ d_i)Â²)

    For embed_dim=30 with 3Ã—SO(10):
        - Full: 30Ã—30 = 900 params
        - Block-diagonal: 3Ã—(10Ã—10) = 300 params
        - Diagonal: 30 params (loses intra-block correlations)
    """

    def __init__(self, blocks: List[torch.Tensor]):
        """
        Args:
            blocks: List of (..., d_i, d_i) covariance matrices
        """
        self.blocks = blocks
        self.n_blocks = len(blocks)
        self.block_dims = [b.shape[-1] for b in blocks]
        self.total_dim = sum(self.block_dims)

    @classmethod
    def from_diagonal(cls, diag: torch.Tensor, irrep_spec: IrrepSpec) -> 'BlockDiagonalCovariance':
        """Create block-diagonal from diagonal (scalar per dimension)."""
        blocks = []
        for start, dim in zip(irrep_spec.block_starts, irrep_spec.block_dims):
            # Extract diagonal for this block, make it a diagonal matrix
            block_diag = diag[..., start:start+dim]
            block = torch.diag_embed(block_diag)
            blocks.append(block)
        return cls(blocks)

    @classmethod
    def from_scalar_per_block(cls, scalars: torch.Tensor, irrep_spec: IrrepSpec) -> 'BlockDiagonalCovariance':
        """Create isotropic blocks: Î£_i = Ïƒ_iÂ² I."""
        blocks = []
        for i, dim in enumerate(irrep_spec.block_dims):
            # scalars[..., i] is variance for block i
            var = scalars[..., i:i+1, None]  # (..., 1, 1)
            I = torch.eye(dim, device=scalars.device, dtype=scalars.dtype)
            block = var * I
            blocks.append(block)
        return cls(blocks)

    def to_full(self) -> torch.Tensor:
        """Convert to full (K, K) matrix. Use sparingly!"""
        batch_shape = self.blocks[0].shape[:-2]
        full = torch.zeros(*batch_shape, self.total_dim, self.total_dim,
                          device=self.blocks[0].device, dtype=self.blocks[0].dtype)
        offset = 0
        for block in self.blocks:
            d = block.shape[-1]
            full[..., offset:offset+d, offset:offset+d] = block
            offset += d
        return full

    def log_det(self) -> torch.Tensor:
        """Compute log|Î£| = Î£_i log|Î£_i|."""
        log_det = 0
        for block in self.blocks:
            # For each block, compute log determinant
            log_det = log_det + torch.linalg.slogdet(block)[1]
        return log_det

    def solve(self, v: torch.Tensor, irrep_spec: IrrepSpec) -> torch.Tensor:
        """Compute Î£â»Â¹ v block-wise."""
        result = torch.zeros_like(v)
        for i, (start, dim) in enumerate(zip(irrep_spec.block_starts, irrep_spec.block_dims)):
            v_block = v[..., start:start+dim]
            # Solve Î£_i x = v_block
            solved = torch.linalg.solve(self.blocks[i], v_block.unsqueeze(-1)).squeeze(-1)
            result[..., start:start+dim] = solved
        return result

    def trace_solve(self, other: 'BlockDiagonalCovariance') -> torch.Tensor:
        """Compute tr(Î£â»Â¹ other) = Î£_i tr(Î£_iâ»Â¹ other_i)."""
        trace = 0
        for my_block, other_block in zip(self.blocks, other.blocks):
            # tr(Aâ»Â¹ B) = tr(solve(A, B))
            solved = torch.linalg.solve(my_block, other_block)
            trace = trace + torch.diagonal(solved, dim1=-2, dim2=-1).sum(dim=-1)
        return trace

    def quadratic_form(self, v: torch.Tensor, irrep_spec: IrrepSpec) -> torch.Tensor:
        """Compute v^T Î£â»Â¹ v block-wise."""
        result = 0
        for i, (start, dim) in enumerate(zip(irrep_spec.block_starts, irrep_spec.block_dims)):
            v_block = v[..., start:start+dim]
            # v^T Î£â»Â¹ v for this block
            solved = torch.linalg.solve(self.blocks[i], v_block.unsqueeze(-1)).squeeze(-1)
            result = result + (v_block * solved).sum(dim=-1)
        return result

    def transform(self, R: torch.Tensor, irrep_spec: IrrepSpec) -> 'BlockDiagonalCovariance':
        """
        Apply rotation: Î£' = R Î£ R^T, block-wise.

        R should be block-diagonal matching irrep structure.
        """
        new_blocks = []
        for i, (start, dim) in enumerate(zip(irrep_spec.block_starts, irrep_spec.block_dims)):
            R_block = R[..., start:start+dim, start:start+dim]
            # Î£'_i = R_i Î£_i R_i^T
            transformed = torch.einsum('...ij,...jk,...lk->...il',
                                       R_block, self.blocks[i], R_block)
            new_blocks.append(transformed)
        return BlockDiagonalCovariance(new_blocks)


@dataclass
class GaussianBelief:
    """
    Gaussian belief q = N(Î¼, Î£) with optional gauge frame Ï†.

    Supports three covariance modes:
    1. Diagonal: sigma is (..., K) - fastest, loses all correlations
    2. Block-diagonal: sigma is BlockDiagonalCovariance - preserves intra-irrep correlations
    3. Full: sigma is (..., K, K) - slowest, full correlations
    """
    mu: torch.Tensor           # (..., K) mean
    sigma: torch.Tensor        # (..., K) diagonal, (..., K, K) full, or BlockDiagonalCovariance
    phi: Optional[torch.Tensor] = None  # (..., dim_g) gauge frame
    irrep_spec: Optional[IrrepSpec] = None  # For block-diagonal mode

    @property
    def is_diagonal(self) -> bool:
        return isinstance(self.sigma, torch.Tensor) and self.sigma.dim() == self.mu.dim()

    @property
    def is_block_diagonal(self) -> bool:
        return isinstance(self.sigma, BlockDiagonalCovariance)

    @property
    def is_full(self) -> bool:
        return isinstance(self.sigma, torch.Tensor) and self.sigma.dim() == self.mu.dim() + 1


def kl_divergence_gaussian(q: GaussianBelief, p: GaussianBelief,
                           transport: Optional[torch.Tensor] = None,
                           irrep_spec: Optional[IrrepSpec] = None) -> torch.Tensor:
    """
    KL divergence KL(q || p) between Gaussians, with optional transport.

    If transport Î© is provided, we compute KL(q || Î©Â·p) where:
        Î©Â·p = N(Î© Î¼_p, Î© Î£_p Î©^T)

    Supports diagonal, block-diagonal, and full covariance modes.

    KL = Â½[tr(Î£_pâ»Â¹ Î£_q) + (Î¼_p - Î¼_q)^T Î£_pâ»Â¹ (Î¼_p - Î¼_q) - K + log|Î£_p|/|Î£_q|]

    Args:
        q: Query belief N(Î¼_q, Î£_q)
        p: Prior/target belief N(Î¼_p, Î£_p)
        transport: Optional (..., K, K) rotation matrix
        irrep_spec: Required for block-diagonal mode

    Returns:
        kl: (...,) KL divergence values
    """
    mu_q = q.mu
    mu_p = p.mu
    sigma_q = q.sigma
    sigma_p = p.sigma

    K = mu_q.shape[-1]

    # Apply transport if provided
    if transport is not None:
        # Rotate prior mean: Î¼_p' = Î© Î¼_p
        mu_p = torch.einsum('...ij,...j->...i', transport, mu_p)

        # Transform covariance based on type
        if q.is_block_diagonal and irrep_spec is not None:
            sigma_p = sigma_p.transform(transport, irrep_spec)
        elif not q.is_diagonal:
            # Full covariance: Î£_p' = Î© Î£_p Î©^T
            sigma_p = torch.einsum('...ij,...jk,...lk->...il',
                                   transport, sigma_p, transport)
        # For diagonal, assume transport preserves diagonal (block structure)

    # Compute KL based on covariance type
    if q.is_diagonal and (isinstance(sigma_p, torch.Tensor) and sigma_p.dim() == mu_p.dim()):
        # Both diagonal - most efficient
        var_q = sigma_q
        var_p = sigma_p

        var_ratio = var_q / (var_p + 1e-8)
        diff = mu_p - mu_q
        mahal = diff ** 2 / (var_p + 1e-8)
        log_det_ratio = torch.log(var_p + 1e-8) - torch.log(var_q + 1e-8)

        kl = 0.5 * (var_ratio + mahal - 1 + log_det_ratio).sum(dim=-1)

    elif q.is_block_diagonal and isinstance(sigma_p, BlockDiagonalCovariance):
        # Both block-diagonal - efficient block-wise computation
        assert irrep_spec is not None, "irrep_spec required for block-diagonal KL"

        # tr(Î£_pâ»Â¹ Î£_q)
        trace_term = sigma_p.trace_solve(sigma_q)

        # (Î¼_p - Î¼_q)^T Î£_pâ»Â¹ (Î¼_p - Î¼_q)
        diff = mu_p - mu_q
        mahal_term = sigma_p.quadratic_form(diff, irrep_spec)

        # log|Î£_p| - log|Î£_q|
        log_det_p = sigma_p.log_det()
        log_det_q = sigma_q.log_det()
        log_det_ratio = log_det_p - log_det_q

        kl = 0.5 * (trace_term + mahal_term - K + log_det_ratio)

    elif q.is_full or (isinstance(sigma_p, torch.Tensor) and sigma_p.dim() == mu_p.dim() + 1):
        # Full covariance
        # tr(Î£_pâ»Â¹ Î£_q)
        Sigma_p_inv_Sigma_q = torch.linalg.solve(sigma_p, sigma_q)
        trace_term = torch.diagonal(Sigma_p_inv_Sigma_q, dim1=-2, dim2=-1).sum(dim=-1)

        # (Î¼_p - Î¼_q)^T Î£_pâ»Â¹ (Î¼_p - Î¼_q)
        diff = (mu_p - mu_q).unsqueeze(-1)
        mahal_term = torch.linalg.solve(sigma_p, diff).squeeze(-1)
        mahal_term = (diff.squeeze(-1) * mahal_term).sum(dim=-1)

        # log|Î£_p| - log|Î£_q|
        log_det_p = torch.linalg.slogdet(sigma_p)[1]
        log_det_q = torch.linalg.slogdet(sigma_q)[1]
        log_det_ratio = log_det_p - log_det_q

        kl = 0.5 * (trace_term + mahal_term - K + log_det_ratio)

    else:
        raise ValueError(f"Incompatible covariance types: q={type(sigma_q)}, p={type(sigma_p)}")

    return kl


# =============================================================================
# PART 4: VARIATIONAL FREE ENERGY FUNCTIONAL
# =============================================================================

class VFEFunctional(nn.Module):
    """
    The Variational Free Energy functional.

    F[q] = Î±Â·F_self + Î²Â·F_align + Î³Â·F_prior + F_obs

    where:
    - F_self = Î£_i H[q_i] (entropy / uncertainty cost)
    - F_align = Î£_{i,j} w_ij Â· KL(q_i || Î©_ij q_j) (belief alignment)
    - F_prior = Î£_i KL(q_i || Ï€_i) (prior divergence)
    - F_obs = Î£_i E_q[-log p(y_i | z_i)] (observation likelihood)
    """

    def __init__(self,
                 embed_dim: int,
                 gauge_dim: int,  # N for SO(N)
                 irrep_spec: Optional[IrrepSpec] = None,
                 alpha: float = 1.0,   # Self-coupling weight
                 beta: float = 1.0,    # Alignment weight
                 gamma: float = 0.1,   # Prior weight
                 bch_order: int = 2,   # BCH truncation
                 temperature: float = 1.0):  # Attention temperature
        super().__init__()

        self.embed_dim = embed_dim
        self.gauge_dim = gauge_dim
        self.irrep_spec = irrep_spec
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.bch_order = bch_order
        self.temperature = temperature

        # Precompute SO(N) structure
        self.register_buffer('generators', so_n_generators(gauge_dim))
        self.register_buffer('f_abc', so_n_structure_constants(gauge_dim))

        self.dim_g = gauge_dim * (gauge_dim - 1) // 2  # Lie algebra dimension

        # For multiple irreps, we need block-diagonal generators
        if irrep_spec is not None:
            self._build_block_diagonal_generators(irrep_spec)

    def _build_block_diagonal_generators(self, irrep_spec: IrrepSpec):
        """
        Build block-diagonal generators for multiple irreps.

        Each block gets its own copy of the SO(N) generators.
        Total embedding dim = sum of block dims.
        """
        # For now, assume all blocks are the same gauge_dim (fundamental rep)
        # Each block of size d gets d(d-1)/2 generators
        K = irrep_spec.total_dim
        n_blocks = irrep_spec.n_blocks

        # Build block-diagonal generators
        # Shape: (dim_g, K, K) where generators are block-diagonal
        block_gens = torch.zeros(self.dim_g, K, K)

        for i, (start, dim) in enumerate(zip(irrep_spec.block_starts, irrep_spec.block_dims)):
            # Only include if this block matches gauge_dim
            if dim == self.gauge_dim:
                block_gens[:, start:start+dim, start:start+dim] = self.generators

        self.register_buffer('block_generators', block_gens)

    def compute_transport(self, phi_i: torch.Tensor, phi_j: torch.Tensor) -> torch.Tensor:
        """
        Compute transport operator Î©_ij = exp(BCH(Ï†_i, -Ï†_j)).

        Args:
            phi_i: (B, N, dim_g) source frames
            phi_j: (B, N, dim_g) target frames

        Returns:
            omega: (B, N, N, gauge_dim, gauge_dim) transport matrices
        """
        B, N, dim_g = phi_i.shape

        # Expand for pairwise computation: (B, N, 1, dim_g) vs (B, 1, N, dim_g)
        phi_i_exp = phi_i.unsqueeze(2)  # (B, N, 1, dim_g)
        phi_j_exp = phi_j.unsqueeze(1)  # (B, 1, N, dim_g)

        # BCH combination: Ï†_ij for all pairs
        phi_ij = bch_combine(phi_i_exp, phi_j_exp, self.f_abc, order=self.bch_order)
        # Shape: (B, N, N, dim_g)

        # Exponentiate to get rotation matrices
        if self.gauge_dim == 3:
            # Use efficient Rodrigues formula for SO(3)
            omega = rodrigues_so3(phi_ij)
        else:
            # General SO(N) via series
            omega = exp_so_n(phi_ij, self.generators)

        return omega  # (B, N, N, gauge_dim, gauge_dim)

    def f_self(self, beliefs: GaussianBelief) -> torch.Tensor:
        """
        Self-coupling term: negative entropy (uncertainty cost).

        H[q] = Â½ log|2Ï€e Î£| = Â½(K log(2Ï€e) + log|Î£|)

        For diagonal: H = Â½ Î£_k log(2Ï€e Ïƒ_k)
        For block-diagonal: H = Â½ Î£_b log|2Ï€e Î£_b|
        """
        if beliefs.is_diagonal:
            # Diagonal case: sum of log variances
            log_det = torch.log(beliefs.sigma + 1e-8).sum(dim=-1)
        elif beliefs.is_block_diagonal:
            # Block-diagonal: sum of block log determinants
            log_det = beliefs.sigma.log_det()
        else:
            # Full covariance: log determinant
            log_det = torch.linalg.slogdet(beliefs.sigma)[1]

        K = beliefs.mu.shape[-1]
        entropy = 0.5 * (K * math.log(2 * math.pi * math.e) + log_det)

        # Return negative entropy (we minimize F, high entropy is good)
        return -entropy.mean()

    def f_align(self, beliefs: GaussianBelief,
                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Belief alignment term: pairwise KL with transport.

        F_align = Î£_{i,j} w_ij Â· KL(q_i || Î©_ij q_j)

        Returns both the free energy and the attention weights (for output).

        For block-diagonal covariance with multiple irreps:
        - Transport is block-diagonal: Î© = diag(Î©_1, Î©_2, ..., Î©_r)
        - Each block transforms independently
        """
        B, N, K = beliefs.mu.shape

        # Compute transport operators (in the gauge_dim space)
        omega_small = self.compute_transport(beliefs.phi, beliefs.phi)
        # Shape: (B, N, N, gauge_dim, gauge_dim)

        # Expand to full embed_dim if using multiple irreps
        if self.irrep_spec is not None and hasattr(self, 'block_generators'):
            # Build block-diagonal transport in full KÃ—K space
            omega = torch.zeros(B, N, N, K, K, device=beliefs.mu.device, dtype=beliefs.mu.dtype)
            for i, (start, dim) in enumerate(zip(self.irrep_spec.block_starts,
                                                   self.irrep_spec.block_dims)):
                if dim == self.gauge_dim:
                    omega[..., start:start+dim, start:start+dim] = omega_small
                else:
                    # Identity for non-matching blocks
                    omega[..., start:start+dim, start:start+dim] = torch.eye(
                        dim, device=omega.device, dtype=omega.dtype)
        else:
            omega = omega_small

        # Expand beliefs for pairwise computation
        mu_i = beliefs.mu.unsqueeze(2)      # (B, N, 1, K)
        mu_j = beliefs.mu.unsqueeze(1)      # (B, 1, N, K)

        # Transport Î¼_j: Î©_ij Î¼_j
        mu_j_transported = torch.einsum('bnmij,bnmj->bnmi', omega, mu_j)

        if beliefs.is_diagonal:
            sigma_i = beliefs.sigma.unsqueeze(2)  # (B, N, 1, K)
            sigma_j = beliefs.sigma.unsqueeze(1)  # (B, 1, N, K)

            # For diagonal covariance, rotation preserves diagonal if isotropic per block
            # Approximation: variance is preserved under rotation
            sigma_j_transported = sigma_j

            # KL(q_i || Î©_ij q_j) - diagonal case
            var_ratio = sigma_i / (sigma_j_transported + 1e-8)
            diff = mu_j_transported - mu_i
            mahal = diff ** 2 / (sigma_j_transported + 1e-8)
            log_det_ratio = torch.log(sigma_j_transported + 1e-8) - torch.log(sigma_i + 1e-8)

            kl_ij = 0.5 * (var_ratio + mahal - 1 + log_det_ratio).sum(dim=-1)

        elif beliefs.is_block_diagonal:
            # Block-diagonal case: compute KL block by block
            # This is more expensive but preserves intra-block correlations
            kl_ij = torch.zeros(B, N, N, device=beliefs.mu.device, dtype=beliefs.mu.dtype)

            for b_idx, (start, dim) in enumerate(zip(self.irrep_spec.block_starts,
                                                      self.irrep_spec.block_dims)):
                # Extract block data
                mu_i_block = mu_i[..., start:start+dim]
                mu_j_block = mu_j_transported[..., start:start+dim]
                sigma_i_block = beliefs.sigma.blocks[b_idx].unsqueeze(2)  # (B, N, 1, d, d)
                sigma_j_block = beliefs.sigma.blocks[b_idx].unsqueeze(1)  # (B, 1, N, d, d)

                # Transform sigma_j block
                omega_block = omega[..., start:start+dim, start:start+dim]
                sigma_j_block = torch.einsum('bnmij,bnmjk,bnmlk->bnmil',
                                             omega_block, sigma_j_block, omega_block)

                # KL for this block
                diff = mu_j_block - mu_i_block
                # tr(Î£_iâ»Â¹ Î£_j)
                trace_term = torch.linalg.solve(sigma_i_block, sigma_j_block)
                trace_term = torch.diagonal(trace_term, dim1=-2, dim2=-1).sum(dim=-1)
                # (Î¼_j - Î¼_i)^T Î£_iâ»Â¹ (Î¼_j - Î¼_i)
                mahal = torch.linalg.solve(sigma_i_block, diff.unsqueeze(-1)).squeeze(-1)
                mahal = (diff * mahal).sum(dim=-1)
                # log|Î£_i| - log|Î£_j|
                log_det_i = torch.linalg.slogdet(sigma_i_block)[1]
                log_det_j = torch.linalg.slogdet(sigma_j_block)[1]

                kl_ij = kl_ij + 0.5 * (trace_term + mahal - dim + log_det_i - log_det_j)

        else:
            # Full covariance case
            raise NotImplementedError("Full covariance f_align not implemented")

        # Convert to attention weights with temperature scaling
        # Lower KL = higher attention
        attn_logits = -kl_ij / self.temperature

        if mask is not None:
            attn_logits = attn_logits.masked_fill(mask == 0, float('-inf'))

        attn_weights = F.softmax(attn_logits, dim=-1)

        # Free energy: expected KL under attention
        f_align = (attn_weights * kl_ij).sum(dim=-1).mean()

        return f_align, attn_weights

    def f_prior(self, beliefs: GaussianBelief,
                priors: GaussianBelief) -> torch.Tensor:
        """
        Prior coupling: KL(q || Ï€).
        """
        kl = kl_divergence_gaussian(beliefs, priors)
        return kl.mean()

    def f_obs(self, beliefs: GaussianBelief,
              targets: torch.Tensor,
              output_proj: nn.Linear) -> torch.Tensor:
        """
        Observation term: cross-entropy prediction loss.

        E_q[-log p(y | z)] â‰ˆ -log p(y | Î¼_q)

        Using point estimate at mean for efficiency.
        """
        # Project belief means to vocabulary
        logits = output_proj(beliefs.mu)  # (B, N, vocab_size)

        # Cross-entropy loss (use reshape for non-contiguous tensors)
        B, N, V = logits.shape
        loss = F.cross_entropy(logits.reshape(-1, V), targets.reshape(-1), reduction='mean')

        return loss

    def forward(self, beliefs: GaussianBelief,
                priors: GaussianBelief,
                targets: torch.Tensor,
                output_proj: nn.Linear,
                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, dict]:
        """
        Compute full VFE.

        Returns:
            vfe: Total free energy
            components: Dict of individual terms
        """
        f_self = self.f_self(beliefs)
        f_align, attn = self.f_align(beliefs, mask)
        f_prior = self.f_prior(beliefs, priors)
        f_obs = self.f_obs(beliefs, targets, output_proj)

        vfe = (self.alpha * f_self +
               self.beta * f_align +
               self.gamma * f_prior +
               f_obs)

        components = {
            'f_self': f_self.item(),
            'f_align': f_align.item(),
            'f_prior': f_prior.item(),
            'f_obs': f_obs.item(),
            'attention': attn,
        }

        return vfe, components


# =============================================================================
# PART 5: BELIEF EMBEDDINGS WITH HAAR INITIALIZATION
# =============================================================================

class BeliefEmbedding(nn.Module):
    """
    Learnable belief embeddings: Î¼, Î£, Ï† for each token.

    Each token t has a belief q_t = N(Î¼_t, Î£_t) in gauge frame Ï†_t.

    Initialization:
    - Î¼: Xavier/Glorot for good gradient flow
    - Î£: Small positive values (log-parameterized)
    - Ï†: Haar measure on SO(N) for symmetry breaking
    """

    def __init__(self,
                 vocab_size: int,
                 embed_dim: int,
                 gauge_dim: int,
                 init_sigma: float = 1.0,
                 init_phi_scale: float = 0.1):
        super().__init__()

        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.gauge_dim = gauge_dim
        self.dim_g = gauge_dim * (gauge_dim - 1) // 2

        # Mean embeddings Î¼
        self.mu = nn.Embedding(vocab_size, embed_dim)

        # Log-variance embeddings (ensures positivity)
        self.log_sigma = nn.Embedding(vocab_size, embed_dim)

        # Gauge frame embeddings Ï† (Lie algebra coefficients)
        self.phi = nn.Embedding(vocab_size, self.dim_g)

        self._init_weights(init_sigma, init_phi_scale)

    def _init_weights(self, init_sigma: float, init_phi_scale: float):
        """Initialize embeddings."""
        # Î¼: Xavier initialization
        nn.init.xavier_uniform_(self.mu.weight)

        # Î£: Initialize to give desired variance
        nn.init.constant_(self.log_sigma.weight, math.log(init_sigma))

        # Ï†: Haar initialization on SO(N)
        # For small angles, Haar measure â‰ˆ uniform on Lie algebra
        # Scale controls typical rotation angle
        nn.init.uniform_(self.phi.weight, -init_phi_scale, init_phi_scale)

    def forward(self, token_ids: torch.Tensor) -> GaussianBelief:
        """
        Look up beliefs for tokens.

        Args:
            token_ids: (B, N) token indices

        Returns:
            beliefs: GaussianBelief with Î¼, Ïƒ, Ï†
        """
        mu = self.mu(token_ids)
        sigma = torch.exp(self.log_sigma(token_ids))  # Ensure positive
        phi = self.phi(token_ids)

        return GaussianBelief(mu=mu, sigma=sigma, phi=phi)


# =============================================================================
# PART 6: Q-FLOW (BELIEF DYNAMICS)
# =============================================================================

class QFlow(nn.Module):
    """
    Q-flow: Fast belief updates via natural gradient descent on VFE.

    dq/dt = -Î· Â· FÌƒâ»Â¹ Â· âˆ‡_q F

    where FÌƒ is the Fisher information metric.

    For Gaussian beliefs, natural gradient has closed form:
    - âˆ‡Ìƒ_Î¼ F = âˆ‡_Î¼ F (Fisher metric is identity for mean)
    - âˆ‡Ìƒ_Î£ F = Î£ Â· âˆ‡_Î£ F Â· Î£ (Fisher metric for covariance)
    """

    def __init__(self,
                 embed_dim: int,
                 n_iterations: int = 1,
                 mu_lr: float = 0.1,
                 sigma_lr: float = 0.01):
        super().__init__()

        self.embed_dim = embed_dim
        self.n_iterations = n_iterations

        # Learnable learning rates (can adapt during training)
        self.mu_lr = nn.Parameter(torch.tensor(mu_lr))
        self.sigma_lr = nn.Parameter(torch.tensor(sigma_lr))

    def step(self, beliefs: GaussianBelief,
             grad_mu: torch.Tensor,
             grad_sigma: torch.Tensor) -> GaussianBelief:
        """
        Single natural gradient step.

        Args:
            beliefs: Current beliefs
            grad_mu: Gradient w.r.t. Î¼
            grad_sigma: Gradient w.r.t. Ïƒ (for diagonal parameterization)

        Returns:
            Updated beliefs
        """
        # Natural gradient for mean (Fisher = I)
        new_mu = beliefs.mu - self.mu_lr * grad_mu

        # Natural gradient for variance
        # For diagonal Î£, Fisher metric gives: âˆ‡Ìƒ_Ïƒ = ÏƒÂ² Â· âˆ‡_Ïƒ
        # Update in log-space for stability
        log_sigma = torch.log(beliefs.sigma + 1e-8)
        new_log_sigma = log_sigma - self.sigma_lr * beliefs.sigma * grad_sigma
        new_sigma = torch.exp(new_log_sigma)

        return GaussianBelief(mu=new_mu, sigma=new_sigma, phi=beliefs.phi)


# =============================================================================
# PART 7: P-FLOW (PRIOR DYNAMICS)
# =============================================================================

class PFlow(nn.Module):
    """
    P-flow: Slow prior updates toward successful beliefs.

    Ï€_t+1 = (1 - Î·) Â· Ï€_t + Î· Â· EMA(q_successful)

    "Successful" beliefs are those that achieved low VFE.
    """

    def __init__(self,
                 embed_dim: int,
                 ema_decay: float = 0.99):
        super().__init__()

        self.embed_dim = embed_dim
        self.ema_decay = ema_decay

    def update(self, priors: GaussianBelief,
               beliefs: GaussianBelief,
               success_weights: Optional[torch.Tensor] = None) -> GaussianBelief:
        """
        Update priors toward successful beliefs.

        Args:
            priors: Current priors
            beliefs: Current beliefs (after Q-flow)
            success_weights: Optional weights indicating belief success

        Returns:
            Updated priors
        """
        if success_weights is None:
            # Uniform weighting
            target_mu = beliefs.mu.mean(dim=1, keepdim=True).expand_as(priors.mu)
            target_sigma = beliefs.sigma.mean(dim=1, keepdim=True).expand_as(priors.sigma)
        else:
            # Weighted average
            weights = success_weights.unsqueeze(-1)
            target_mu = (beliefs.mu * weights).sum(dim=1, keepdim=True) / weights.sum(dim=1, keepdim=True)
            target_sigma = (beliefs.sigma * weights).sum(dim=1, keepdim=True) / weights.sum(dim=1, keepdim=True)

        # EMA update
        new_mu = self.ema_decay * priors.mu + (1 - self.ema_decay) * target_mu
        new_sigma = self.ema_decay * priors.sigma + (1 - self.ema_decay) * target_sigma

        return GaussianBelief(mu=new_mu, sigma=new_sigma, phi=priors.phi)


# =============================================================================
# PART 8: FULL FEP TRANSFORMER
# =============================================================================

class FEPTransformer(nn.Module):
    """
    Full Free Energy Principle Transformer.

    Architecture:
    1. Token â†’ Belief embedding (Î¼, Î£, Ï†)
    2. Q-flow: Minimize VFE via natural gradient (attention emerges)
    3. P-flow: Update priors toward successful beliefs
    4. Output: Project final beliefs to vocabulary

    NO learned attention weights, NO MLP layers.
    Everything emerges from VFE minimization.
    """

    def __init__(self,
                 vocab_size: int,
                 embed_dim: int,
                 gauge_dim: int,
                 irrep_spec: Optional[List[Tuple[str, int, int]]] = None,
                 n_layers: int = 1,
                 n_q_iterations: int = 5,
                 alpha: float = 0.1,
                 beta: float = 1.0,
                 gamma: float = 0.1,
                 bch_order: int = 2,
                 temperature: float = 1.0,
                 tie_embeddings: bool = False):
        super().__init__()

        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.gauge_dim = gauge_dim
        self.n_layers = n_layers

        # Parse irrep specification
        if irrep_spec is not None:
            self.irrep_spec = IrrepSpec(irrep_spec)
            assert self.irrep_spec.total_dim == embed_dim, \
                f"Irrep spec total dim {self.irrep_spec.total_dim} != embed_dim {embed_dim}"
        else:
            # Default: single block matching gauge_dim
            self.irrep_spec = None

        # Belief embeddings
        self.belief_embed = BeliefEmbedding(vocab_size, embed_dim, gauge_dim)

        # Prior bank (learnable priors per token)
        self.prior_embed = BeliefEmbedding(vocab_size, embed_dim, gauge_dim)

        # VFE functional
        self.vfe = VFEFunctional(embed_dim, gauge_dim, self.irrep_spec,
                                  alpha, beta, gamma, bch_order, temperature)

        # Q-flow dynamics
        self.q_flow = QFlow(embed_dim, n_iterations=n_q_iterations)

        # P-flow dynamics
        self.p_flow = PFlow(embed_dim)

        # Output projection
        self.output_proj = nn.Linear(embed_dim, vocab_size, bias=False)

        if tie_embeddings:
            self.output_proj.weight = self.belief_embed.mu.weight

    def _run_q_flow(self, beliefs: GaussianBelief, priors: GaussianBelief,
                    mask: torch.Tensor) -> Tuple[GaussianBelief, torch.Tensor, torch.Tensor]:
        """
        Run Q-flow iterations to optimize beliefs.

        Returns updated beliefs, final vfe_internal, and attention weights.
        """
        for q_iter in range(self.q_flow.n_iterations):
            # Ensure gradients can be computed
            beliefs.mu.requires_grad_(True)
            beliefs.sigma.requires_grad_(True)

            f_self = self.vfe.f_self(beliefs)
            f_align, attn = self.vfe.f_align(beliefs, mask)
            f_prior = self.vfe.f_prior(beliefs, priors)

            # Internal VFE for optimization (NO f_obs!)
            vfe_internal = (self.vfe.alpha * f_self +
                           self.vfe.beta * f_align +
                           self.vfe.gamma * f_prior)

            # Compute gradients w.r.t. beliefs
            grads = torch.autograd.grad(
                vfe_internal,
                [beliefs.mu, beliefs.sigma],
                create_graph=self.training,
                retain_graph=True
            )

            # Update beliefs via natural gradient
            beliefs = self.q_flow.step(beliefs, grads[0], grads[1])

        return beliefs, vfe_internal, attn

    def forward(self,
                input_ids: torch.Tensor,
                targets: Optional[torch.Tensor] = None,
                return_components: bool = False) -> dict:
        """
        Forward pass.

        Args:
            input_ids: (B, N) input token IDs
            targets: (B, N) target token IDs (for training)
            return_components: Whether to return VFE components

        Returns:
            Dict with logits, loss, and optionally components
        """
        B, N = input_ids.shape

        # Get initial beliefs from embeddings
        beliefs = self.belief_embed(input_ids)
        priors = self.prior_embed(input_ids)

        # Causal mask for autoregressive
        mask = torch.tril(torch.ones(N, N, device=input_ids.device)).unsqueeze(0)

        # Q-flow: iterative belief updates
        # CRITICAL: Do NOT include targets during Q-flow to prevent label leakage
        # The model must optimize beliefs based ONLY on self-consistency and priors

        # Check if we're in inference mode (no gradients available)
        inference_mode = not torch.is_grad_enabled()

        if inference_mode:
            # During inference, run Q-flow with gradient computation enabled
            with torch.enable_grad():
                beliefs, vfe_internal, attn = self._run_q_flow(beliefs, priors, mask)
        else:
            # During training, run normally
            beliefs, vfe_internal, attn = self._run_q_flow(beliefs, priors, mask)

        # Output logits (AFTER Q-flow has finished)
        logits = self.output_proj(beliefs.mu)

        # Components for logging
        components = {'attention': attn, 'vfe_internal': vfe_internal.item()}

        result = {'logits': logits}

        if targets is not None:
            # Compute observation loss AFTER Q-flow (no label leakage!)
            f_obs = F.cross_entropy(
                logits.reshape(-1, self.vocab_size),
                targets.reshape(-1)
            )
            components['f_obs'] = f_obs.item()

            # Total VFE = internal terms + observation
            # The internal terms (f_self, f_align, f_prior) were already optimized by Q-flow
            # f_obs provides the learning signal
            result['loss'] = vfe_internal + f_obs
            result['ce_loss'] = f_obs

        if return_components:
            result['components'] = components
            result['attention'] = components.get('attention')

        return result

    def generate(self,
                 input_ids: torch.Tensor,
                 max_new_tokens: int = 50,
                 temperature: float = 1.0) -> torch.Tensor:
        """
        Autoregressive generation.
        """
        for _ in range(max_new_tokens):
            # Get predictions for current sequence
            outputs = self.forward(input_ids)
            logits = outputs['logits'][:, -1, :] / temperature

            # Sample next token
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)

            # Append to sequence
            input_ids = torch.cat([input_ids, next_token], dim=1)

        return input_ids


# =============================================================================
# PART 9: TESTING / SANITY CHECKS
# =============================================================================

def test_so_n_algebra():
    """Test SO(N) Lie algebra implementation."""
    print("Testing SO(N) Lie algebra...")

    for n in [3, 5, 10]:
        generators = so_n_generators(n)
        f_abc = so_n_structure_constants(n)

        n_gen = n * (n - 1) // 2
        assert generators.shape == (n_gen, n, n), f"Wrong generator shape for SO({n})"
        assert f_abc.shape == (n_gen, n_gen, n_gen), f"Wrong f_abc shape for SO({n})"

        # Check antisymmetry: f_abc = -f_bac
        assert torch.allclose(f_abc, -f_abc.transpose(0, 1), atol=1e-6), \
            f"Structure constants not antisymmetric for SO({n})"

        # Check Jacobi identity: f_abe f_ecd + f_bce f_ead + f_cae f_ebd = 0
        jacobi = torch.einsum('abe,ecd->abcd', f_abc, f_abc) + \
                 torch.einsum('bce,ead->abcd', f_abc, f_abc) + \
                 torch.einsum('cae,ebd->abcd', f_abc, f_abc)
        assert torch.allclose(jacobi, torch.zeros_like(jacobi), atol=1e-5), \
            f"Jacobi identity violated for SO({n})"

        print(f"  SO({n}): {n_gen} generators, Jacobi âœ“")

    print("SO(N) algebra tests passed!\n")


def test_bch():
    """Test BCH formula."""
    print("Testing BCH formula...")

    n = 3  # SO(3)
    f_abc = so_n_structure_constants(n)

    # For small angles, BCH should be approximately additive
    phi_1 = torch.randn(3) * 0.1
    phi_2 = torch.randn(3) * 0.1

    bch_1 = bch_combine(phi_1, phi_2, f_abc, order=1)
    bch_2 = bch_combine(phi_1, phi_2, f_abc, order=2)

    # Order 1 should just be sum
    assert torch.allclose(bch_1, phi_1 - phi_2), "BCH order 1 should be simple sum"

    # Order 2 should have commutator correction
    bracket = lie_bracket(phi_1, phi_2, f_abc)
    expected = phi_1 - phi_2 - 0.5 * bracket
    assert torch.allclose(bch_2, expected), "BCH order 2 incorrect"

    print("  BCH formula tests passed!\n")


def test_rodrigues():
    """Test Rodrigues formula."""
    print("Testing Rodrigues formula...")

    # Small rotation
    phi = torch.tensor([0.1, 0.2, 0.3])
    R = rodrigues_so3(phi.unsqueeze(0)).squeeze(0)

    # Check orthogonality
    should_be_I = R @ R.T
    assert torch.allclose(should_be_I, torch.eye(3), atol=1e-5), "R not orthogonal"

    # Check determinant = 1
    det = torch.linalg.det(R)
    assert torch.allclose(det, torch.tensor(1.0), atol=1e-5), "det(R) != 1"

    # Compare with matrix exponential
    generators = so_n_generators(3)
    R_exp = exp_so_n(phi.unsqueeze(0), generators, max_terms=10).squeeze(0)
    assert torch.allclose(R, R_exp, atol=1e-4), "Rodrigues != matrix exp"

    print("  Rodrigues formula tests passed!\n")


if __name__ == '__main__':
    test_so_n_algebra()
    test_bch()
    test_rodrigues()

    print("Creating FEP Transformer...")
    model = FEPTransformer(
        vocab_size=1000,
        embed_dim=10,  # Small for testing
        gauge_dim=10,  # SO(10)
        n_layers=1,
        n_q_iterations=3,
    )

    # Test forward pass
    x = torch.randint(0, 1000, (2, 16))  # Batch of 2, seq len 16
    y = torch.randint(0, 1000, (2, 16))

    output = model(x, y, return_components=True)
    print(f"Logits shape: {output['logits'].shape}")
    print(f"Loss: {output['loss'].item():.4f}")
    print(f"CE Loss: {output['ce_loss'].item():.4f}")
    print(f"Attention shape: {output['attention'].shape}")

    print("\nAll tests passed!")
