# Pure FEP Transformer: Ground-Up Implementation Plan (REVISED)

## Executive Summary

This document provides a detailed plan to implement a **Pure Free Energy Principle (FEP) Transformer** from first principles, with **gauge frames as the core semantic/feature encoding mechanism**.

**Key Design Decisions:**
1. **Gauge Frames for Semantic Encoding**: Ï† encodes semantic/feature structure, NOT position
2. **Full Transport Operators**: Î©_ij = exp(Ï†_i)Â·exp(-Ï†_j) is used in ALL KL terms
3. **Complete VFE with Prior Coupling**: Includes the Î³_ijÂ·KL(p_i||Î©_ijÂ·p_j) term
4. **Position via Priors Only**: Position-dependent priors (Î¼_p, Ïƒ_p), NO position in Ï†
5. **Two Timescales**: Q-flow (beliefs), P-flow (priors), and **Ï†-flow** (gauge frames)
6. **No Neural Networks**: Zero MLPs, zero learned projection matrices, zero activation functions

---

## I. Theoretical Foundation

### 1.1 The COMPLETE Variational Free Energy Functional

From the papers, the FULL VFE is:

```
F[{q_i}, {p_i}, {Ï†_i}] =
    Î± Â· Î£_i KL(q_i || p_i)                           [Self-coupling: belief-to-prior]
  + Î»_Î² Â· Î£_ij Î²_ij Â· KL(q_i || Î©_ijÂ·q_j)           [Belief alignment with transport]
  + Î»_Î³ Â· Î£_ij Î³_ij Â· KL(p_i || Î©_ijÂ·p_j)           [Prior coupling with transport]
  - Î£_i E_{q_i}[log p(y_i | z_i)]                    [Observation likelihood]
```

where:
- **Î©_ij = exp(Ï†_i) Â· exp(-Ï†_j)** is the gauge transport operator
- **Î²_ij** are belief attention weights
- **Î³_ij** are prior (model) attention weights
- **Ï†_i âˆˆ ğ”¤** are gauge frames in the Lie algebra

### 1.2 Why Gauge Frames are ESSENTIAL

The gauge frames Ï†_i encode the **semantic reference frame** of each agent/token:

1. **Semantic Orientation**: Ï† encodes HOW a token "sees" the embedding space
2. **Feature Encoding**: Different tokens have different Ï†, encoding their semantic role
3. **Transport = Communication**: Î©_ij transforms j's beliefs into i's frame for comparison
4. **Multi-Head from Lie Algebra**: For SO(3), dim(ğ”¤) = 3 gives 3 natural heads

**Critical Distinction**:
- **Ï† encodes WHAT** (semantic features, token identity)
- **Position priors encode WHERE** (sequence position)

### 1.3 The Transport Operator

For gauge group G (typically SO(3) or SO(N)), with generators {G_a}:

```
Ï†_i = Î£_a Ï†_i^(a) Â· G_a    âˆˆ ğ”¤ (Lie algebra)

Î©_ij = exp(Ï†_i) Â· exp(-Ï†_j)  âˆˆ G (Lie group)
```

The transport acts on Gaussian statistics:
```
Î©_ij Â· N(Î¼_j, Î£_j) = N(Î©_ij Â· Î¼_j, Î©_ij Â· Î£_j Â· Î©_ij^T)
```

For diagonal covariances with efficient transport:
```
(Î© Â· diag(Ïƒ) Â· Î©^T)_kk = Î£_l Î©_klÂ² Â· Ïƒ_l
```

### 1.4 Attention as Transported Belief Alignment

Attention weights emerge from KL divergence **after transport**:

```
Î²_ij = softmax_j(-KL(q_i || Î©_ijÂ·q_j) / Îº_Î²)
Î³_ij = softmax_j(-KL(p_i || Î©_ijÂ·p_j) / Îº_Î³)
```

**Why transport matters for attention:**
- Without transport: comparing apples to oranges
- With transport: align j's frame to i's frame, THEN compare
- Tokens with aligned frames (small ||Ï†_i - Ï†_j||) have easier communication

### 1.5 Multi-Head Attention from Lie Algebra

For G = SO(3), the Lie algebra ğ”¤ = so(3) has dimension 3:
```
Number of heads H = dim(ğ”¤) = 3
```

Each generator G_a defines a rotation axis, creating 3 natural attention heads:
- Head 1: Rotations around x-axis
- Head 2: Rotations around y-axis
- Head 3: Rotations around z-axis

The embedding space decomposes via irreducible representations (irreps):
```
K = n_0Â·1 + n_1Â·3 + n_2Â·5 + ...
    [scalars] [vectors] [rank-2 tensors]
```

For K=64: could use 10 scalars + 18 vectors = 10Â·1 + 18Â·3 = 64

---

## II. Core Architecture

### 2.1 Agent Representation (Complete)

Each token position i has a **full section** of the bundle:

```
Agent i = (q_i, p_i, Ï†_i)

where:
  q_i = N(Î¼_qi, Ïƒ_qiÂ²)   - belief (posterior)
  p_i = N(Î¼_pi, Ïƒ_piÂ²)   - prior (generative model)
  Ï†_i âˆˆ â„^{phi_dim}      - gauge frame (semantic orientation)
```

**Shapes:**
```
Î¼_q:    (batch, seq_len, embed_dim)  - belief means
Ïƒ_q:    (batch, seq_len, embed_dim)  - belief stds
Î¼_p:    (seq_len, embed_dim)         - position prior means
Ïƒ_p:    (seq_len, embed_dim)         - position prior stds
Ï†:      (batch, seq_len, phi_dim)    - gauge frames
```

For SO(3): phi_dim = 3
For SO(N): phi_dim = N(N-1)/2

### 2.2 Token Prior Bank (with Gauge Frames!)

Each vocabulary token v has a **complete prior section**:

```python
class TokenPriorBank:
    """
    Each token v has: Ï€_v = (Î¼_v, Ïƒ_v, Ï†_v)

    The gauge frame Ï†_v encodes the token's SEMANTIC orientation.
    Different tokens "see" the embedding space from different angles.
    """
    Î¼_tokens: (vocab_size, embed_dim)   # semantic content
    Ïƒ_tokens: (vocab_size, embed_dim)   # uncertainty
    Ï†_tokens: (vocab_size, phi_dim)     # semantic frame
```

**Encoding**: Initialize agent from token prior:
```
q_i â† N(Î¼_{token[i]}, Ïƒ_{token[i]})
Ï†_i â† Ï†_{token[i]}
```

**Decoding**: Output via transported KL:
```
logits_v = -KL(q_i || Î©_{iv}Â·Ï€_v) / Ï„

where Î©_{iv} = exp(Ï†_i)Â·exp(-Ï†_v) transports token prior to agent's frame
```

### 2.3 Position-Dependent Priors (NO Ï† for position!)

Position is encoded in priors, NOT in gauge frames:

```python
class PositionPriors:
    """
    Position structure in (Î¼_p, Ïƒ_p) only.
    Gauge frames Ï† are for SEMANTIC encoding.
    """
    Î¼_p: (max_seq_len, embed_dim)   # position-dependent means
    Ïƒ_p: (max_seq_len, embed_dim)   # position-dependent stds
    # NO Ï†_position!
```

**Why this separation?**
- Ï† should be **shift-invariant** (same token â†’ same Ï† regardless of position)
- Position structure emerges from (Î¼_p, Ïƒ_p) learning different patterns
- Transport Î©_ij depends on semantic frames, not position

### 2.4 Single Layer Structure

```
Input: (Î¼_q, Ïƒ_q, Ï†) from previous layer or token encoding
       (Î¼_p, Ïƒ_p) position priors for this layer

1. COMPUTE TRANSPORT OPERATORS:
   For all pairs (i,j):
     Î©_ij = exp(Ï†_iÂ·G) Â· exp(-Ï†_jÂ·G)

2. COMPUTE ATTENTION (from transported beliefs):
   KL_ij = KL(q_i || Î©_ijÂ·q_j)
   Î²_ij = softmax_j(-KL_ij / Îº_Î²)

3. VFE GRADIENT DESCENT (Q-flow):
   for step in range(n_vfe_steps):
     F = Î±Â·Î£ KL(q||p) + Î»_Î²Â·Î£ Î²Â·KL(q||Î©Â·q) + Î»_Î³Â·Î£ Î³Â·KL(p||Î©Â·p) - log p(y|q)

     # Natural gradient updates
     Î¼_q â† Î¼_q - Î·_Î¼ Â· Ïƒ_qÂ² Â· âˆ‚F/âˆ‚Î¼_q
     Ïƒ_q â† Ïƒ_q Â· exp(-Î·_Ïƒ Â· âˆ‚F/âˆ‚log_Ïƒ_q)
     Ï† â† Ï† - Î·_Ï† Â· âˆ‚F/âˆ‚Ï†

     # Optionally recompute Î² (dynamic attention)
     Î²_ij = softmax(-KL(q_i||Î©_ijÂ·q_j) / Îº_Î²)

Output: (Î¼_q, Ïƒ_q, Ï†) updated beliefs and frames
```

---

## III. The VFE Components in Detail

### 3.1 Self-Coupling: KL(q_i || p_i)

Standard diagonal Gaussian KL (no transport needed - same agent):

```python
def kl_self_coupling(Î¼_q, Ïƒ_q, Î¼_p, Ïƒ_p, eps=1e-6):
    """KL(q || p) for diagonal Gaussians."""
    var_q = Ïƒ_q.square() + eps
    var_p = Ïƒ_p.square() + eps

    kl = 0.5 * (
        torch.log(var_p / var_q)
        + var_q / var_p
        + (Î¼_q - Î¼_p).square() / var_p
        - 1.0
    )
    return kl.sum(dim=-1)  # (B, N)
```

### 3.2 Belief Alignment: Î£ Î²_ij Â· KL(q_i || Î©_ijÂ·q_j)

**WITH GAUGE TRANSPORT**:

```python
def compute_transport_operators(phi, generators):
    """
    Compute Î©_ij = exp(Ï†_iÂ·G)Â·exp(-Ï†_jÂ·G) for all pairs.

    Args:
        phi: (B, N, phi_dim) gauge frames
        generators: (phi_dim, K, K) Lie algebra generators

    Returns:
        Î©: (B, N, N, K, K) transport operators
    """
    B, N, phi_dim = phi.shape
    K = generators.shape[1]

    # Compute exp(Ï†Â·G) for each agent
    phi_dot_G = torch.einsum('bna,aij->bnij', phi, generators)  # (B, N, K, K)
    R = torch.linalg.matrix_exp(phi_dot_G)  # (B, N, K, K)

    # Î©_ij = R_i @ R_j^T
    Omega = torch.einsum('bnik,bnjk->bnijk', R, R)  # (B, N, N, K, K)
    # Note: R_j^T = inv(R_j) for orthogonal matrices

    return Omega

def kl_transported(Î¼_q, Ïƒ_q, Omega, eps=1e-6):
    """
    KL(q_i || Î©_ijÂ·q_j) for all pairs.

    Transported belief: Î©_ijÂ·q_j = N(Î©_ijÂ·Î¼_j, Î©_ijÂ·Î£_jÂ·Î©_ij^T)
    """
    B, N, K = Î¼_q.shape
    var_q = Ïƒ_q.square() + eps  # (B, N, K)

    # Transport means: Î©_ij @ Î¼_j
    Î¼_transported = torch.einsum('bnijk,bjk->bnik', Omega, Î¼_q)  # (B, N, N, K)

    # Transport variances (diagonal): (Î© @ diag(ÏƒÂ²) @ Î©^T)_kk = Î£_l Î©_klÂ² Â· Ïƒ_lÂ²
    var_transported = torch.einsum('bnijk,bjk,bnijk->bnik',
                                    Omega, var_q, Omega)  # (B, N, N, K)

    # KL(q_i || transported_j)
    Î¼_i = Î¼_q.unsqueeze(2)  # (B, N, 1, K)
    var_i = var_q.unsqueeze(2)  # (B, N, 1, K)

    kl = 0.5 * (
        torch.log(var_transported / var_i)
        + var_i / var_transported
        + (Î¼_i - Î¼_transported).square() / var_transported
        - 1.0
    )
    return kl.sum(dim=-1)  # (B, N, N)

def compute_attention(kl_matrix, kappa, mask=None):
    """Î²_ij = softmax_j(-KL_ij / Îº)"""
    logits = -kl_matrix / kappa
    if mask is not None:
        logits = logits.masked_fill(~mask, float('-inf'))
    return F.softmax(logits, dim=-1)
```

### 3.3 Prior Coupling: Î£ Î³_ij Â· KL(p_i || Î©_ijÂ·p_j)

**THE MISSING TERM** - ensures priors form a coherent world model:

```python
def prior_coupling_term(Î¼_p, Ïƒ_p, Omega, kappa_gamma, mask=None):
    """
    Î£ Î³_ij Â· KL(p_i || Î©_ijÂ·p_j)

    This term ensures priors are mutually consistent under transport.
    """
    # Compute KL between priors with transport
    kl_priors = kl_transported_priors(Î¼_p, Ïƒ_p, Omega)  # (N, N)

    # Compute Î³ attention weights
    gamma = compute_attention(kl_priors, kappa_gamma, mask)  # (N, N)

    # Weighted sum
    prior_coupling = (gamma * kl_priors).sum()

    return prior_coupling, gamma
```

### 3.4 Observation Likelihood

Output via **transported KL** to token priors:

```python
def observation_likelihood(Î¼_q, Ïƒ_q, Ï†, token_priors, tau=1.0):
    """
    logits_v = -KL(q_i || Î©_{iv}Â·Ï€_v) / Ï„

    Transport each token prior into the agent's frame before comparing.
    """
    B, N, K = Î¼_q.shape
    V = token_priors.Î¼_tokens.shape[0]

    # Compute transport from each agent to each token prior
    # Î©_{iv} = exp(Ï†_i)Â·exp(-Ï†_v)
    Omega_to_tokens = compute_agent_to_token_transport(
        Ï†, token_priors.Ï†_tokens, generators
    )  # (B, N, V, K, K)

    # Transport token priors
    Î¼_transported = transport_means(token_priors.Î¼_tokens, Omega_to_tokens)
    Ïƒ_transported = transport_stds(token_priors.Ïƒ_tokens, Omega_to_tokens)

    # KL to each transported token prior
    kl_to_tokens = compute_kl_batch(Î¼_q, Ïƒ_q, Î¼_transported, Ïƒ_transported)

    # Logits
    logits = -kl_to_tokens / tau  # (B, N, V)

    return logits
```

### 3.5 Complete VFE Computation

```python
def compute_vfe(Î¼_q, Ïƒ_q, Ï†, Î¼_p, Ïƒ_p, Omega, target_ids, token_priors, config):
    """
    FULL Variational Free Energy:

    F = Î±Â·Î£_i KL(q_i||p_i)
      + Î»_Î²Â·Î£_ij Î²_ijÂ·KL(q_i||Î©_ijÂ·q_j)
      + Î»_Î³Â·Î£_ij Î³_ijÂ·KL(p_i||Î©_ijÂ·p_j)
      - Î£_i log p(y_i|q_i)
    """
    # 1. Self-coupling
    kl_self = kl_self_coupling(Î¼_q, Ïƒ_q, Î¼_p, Ïƒ_p)
    F_self = config.alpha * kl_self.sum()

    # 2. Belief alignment (WITH TRANSPORT)
    kl_beliefs = kl_transported(Î¼_q, Ïƒ_q, Omega)
    beta = compute_attention(kl_beliefs, config.kappa_beta, mask)
    F_belief = config.lambda_beta * (beta * kl_beliefs).sum()

    # 3. Prior coupling (WITH TRANSPORT) - THE MISSING TERM!
    kl_priors = kl_transported_priors(Î¼_p, Ïƒ_p, Omega)
    gamma = compute_attention(kl_priors, config.kappa_gamma, mask)
    F_prior = config.lambda_gamma * (gamma * kl_priors).sum()

    # 4. Observation likelihood
    logits = observation_likelihood(Î¼_q, Ïƒ_q, Ï†, token_priors, config.tau)
    ce_loss = F.cross_entropy(logits.view(-1, V), target_ids.view(-1))
    F_obs = ce_loss * target_ids.numel()

    F_total = F_self + F_belief + F_prior + F_obs

    return F_total, {
        'F_self': F_self.item(),
        'F_belief': F_belief.item(),
        'F_prior': F_prior.item(),
        'F_obs': F_obs.item(),
        'beta': beta,
        'gamma': gamma
    }
```

---

## IV. Gradient Computation (including âˆ‚F/âˆ‚Ï†!)

### 4.1 Gradient with respect to Gauge Frames: âˆ‚F/âˆ‚Ï†_i

This is CRUCIAL - gauge frames evolve via VFE gradient descent:

```python
def compute_phi_gradient(Ï†, Î¼_q, Ïƒ_q, Î¼_p, Ïƒ_p, Î², Î³, generators, config):
    """
    âˆ‚F/âˆ‚Ï†_i includes contributions from:
    1. Belief alignment: Î£_j [âˆ‚Î²_ij/âˆ‚Ï†_i Â· KL_ij + Î²_ij Â· âˆ‚KL_ij/âˆ‚Ï†_i]
    2. Prior coupling:   Î£_j [âˆ‚Î³_ij/âˆ‚Ï†_i Â· KL_ij^p + Î³_ij Â· âˆ‚KL_ij^p/âˆ‚Ï†_i]
    3. Others to me:     Î£_k Î²_ki Â· âˆ‚KL(q_k||Î©_kiÂ·q_i)/âˆ‚Ï†_i
    4. Priors others:    Î£_k Î³_ki Â· âˆ‚KL(p_k||Î©_kiÂ·p_i)/âˆ‚Ï†_i

    The gradient flows through the transport operator Î©_ij = exp(Ï†_i)Â·exp(-Ï†_j)
    """
    # Use autograd for correctness, then optimize if needed
    Ï†.requires_grad_(True)

    # Recompute F with gradient tracking
    Omega = compute_transport_operators(Ï†, generators)
    F, _ = compute_vfe(Î¼_q, Ïƒ_q, Ï†, Î¼_p, Ïƒ_p, Omega, ...)

    # Gradient via autograd
    grad_phi = torch.autograd.grad(F, Ï†, retain_graph=True)[0]

    return grad_phi
```

### 4.2 Three-Timescale Updates

```python
def vfe_step(Î¼_q, Ïƒ_q, Ï†, Î¼_p, Ïƒ_p, generators, config):
    """
    Single VFE gradient descent step updating:
    - Î¼_q (belief means) - fast
    - Ïƒ_q (belief stds) - fast
    - Ï† (gauge frames) - medium (can be slower than beliefs)
    """
    # Compute transport operators
    Omega = compute_transport_operators(Ï†, generators)

    # Compute VFE and all gradients
    with torch.enable_grad():
        Î¼_q.requires_grad_(True)
        Ïƒ_q.requires_grad_(True)
        Ï†.requires_grad_(True)

        F, metrics = compute_vfe(Î¼_q, Ïƒ_q, Ï†, Î¼_p, Ïƒ_p, Omega, ...)

        grad_Î¼ = torch.autograd.grad(F, Î¼_q, retain_graph=True)[0]
        grad_Ïƒ = torch.autograd.grad(F, Ïƒ_q, retain_graph=True)[0]
        grad_Ï† = torch.autograd.grad(F, Ï†)[0]

    # Natural gradient updates
    var_q = Ïƒ_q.square()
    Î¼_q_new = Î¼_q - config.lr_mu * var_q * grad_Î¼
    Ïƒ_q_new = Ïƒ_q * torch.exp(-config.lr_sigma * grad_Ïƒ * Ïƒ_q)
    Ï†_new = Ï† - config.lr_phi * grad_Ï†

    # Clamp for stability
    Ïƒ_q_new = Ïƒ_q_new.clamp(min=config.variance_floor)
    Ï†_new = clamp_phi_norm(Ï†_new, config.phi_max_norm)  # e.g., Ï€

    return Î¼_q_new, Ïƒ_q_new, Ï†_new
```

---

## V. Why Semantic Encoding in Ï† Works

### 5.1 Token Identity via Gauge Frame

Different tokens have different "orientations" in semantic space:

```
Token "cat" â†’ Ï†_cat = [0.3, -0.1, 0.5]   (some orientation)
Token "dog" â†’ Ï†_dog = [0.4, -0.2, 0.6]   (similar orientation - similar semantics!)
Token "run" â†’ Ï†_run = [-0.5, 0.8, 0.1]   (different orientation - different category)
```

When computing attention:
- cat attending to dog: Î©_{cat,dog} â‰ˆ I (small rotation, easy transport)
- cat attending to run: Î©_{cat,run} = large rotation (harder transport)

This creates **semantic clustering** in attention patterns!

### 5.2 Transport Cost as Semantic Distance

The KL divergence after transport:
```
KL(q_cat || Î©_{cat,run}Â·q_run)
```
includes an implicit cost for the transport itself. Even if the beliefs (Î¼, Ïƒ) are similar, if the frames are misaligned, attention is reduced.

### 5.3 Multi-Head = Multiple Semantic Axes

For SO(3) with 3 generators:
- Head 1 (G_x): Captures one axis of semantic variation
- Head 2 (G_y): Captures another axis
- Head 3 (G_z): Captures third axis

Different heads attend to different aspects of semantic similarity.

---

## VI. Position Encoding (WITHOUT Ï†)

### 6.1 Position in Priors Only

```python
class LayerPriors:
    """
    Position structure emerges from position-dependent priors.
    NOT from gauge frames.
    """
    def __init__(self, max_seq_len, embed_dim):
        # Position-dependent prior means
        self.Î¼_p = nn.Parameter(torch.randn(max_seq_len, embed_dim) * 0.1)
        # Position-dependent prior stds
        self.log_Ïƒ_p = nn.Parameter(torch.zeros(max_seq_len, embed_dim))

        # NO Ï†_position - gauge frames come from TOKEN priors only!
```

### 6.2 Why Position Emerges

Through P-flow, position priors learn:
- Position 0 sees beginning-of-sequence patterns
- Position N-1 sees end-of-sequence patterns
- Middle positions learn their characteristic patterns

The causal mask ensures positional asymmetry. Priors naturally differentiate.

---

## VII. Complete Model Architecture

### 7.1 Configuration

```python
@dataclass
class PureFEPConfig:
    # Architecture
    vocab_size: int = 256
    embed_dim: int = 64           # K
    n_layers: int = 4
    max_seq_len: int = 128

    # Gauge structure
    gauge_group: str = 'SO3'      # 'SO3' or 'SON'
    phi_dim: int = 3              # dim(ğ”¤): 3 for SO(3), N(N-1)/2 for SO(N)
    n_heads: int = 3              # = phi_dim for SO(3)

    # VFE weights
    alpha: float = 0.1            # Self-coupling
    lambda_beta: float = 1.0      # Belief alignment
    lambda_gamma: float = 0.1     # Prior coupling (NEW!)
    kappa_beta: float = 1.0       # Belief attention temperature
    kappa_gamma: float = 1.0      # Prior attention temperature
    tau: float = 1.0              # Output temperature

    # Q-flow (fast timescale)
    n_vfe_steps: int = 10
    lr_mu: float = 0.1
    lr_sigma: float = 0.01
    lr_phi: float = 0.05          # Gauge frame learning rate

    # P-flow (slow timescale)
    lr_prior: float = 0.01
    lr_token_prior: float = 0.01

    # Stability
    variance_floor: float = 1e-4
    phi_max_norm: float = 3.14159  # Ï€ radians
    eps: float = 1e-6
```

### 7.2 Model Definition

```python
class PureFEPTransformer(nn.Module):
    """
    Pure FEP Transformer with FULL gauge structure.

    - Gauge frames Ï† encode SEMANTIC features
    - Transport Î©_ij = exp(Ï†_i)Â·exp(-Ï†_j) in ALL KL terms
    - Position encoded in priors (Î¼_p, Ïƒ_p), NOT in Ï†
    - Complete VFE includes prior coupling term
    """

    def __init__(self, config: PureFEPConfig):
        super().__init__()
        self.config = config

        # Generate Lie algebra generators
        if config.gauge_group == 'SO3':
            self.generators = generate_so3_generators()  # (3, K, K)
        else:
            self.generators = generate_soN_generators(config.phi_dim)
        self.register_buffer('generators_buf', self.generators)

        # Token prior bank (Î¼, Ïƒ, Ï† for each token)
        self.token_priors = TokenPriorBank(
            vocab_size=config.vocab_size,
            embed_dim=config.embed_dim,
            phi_dim=config.phi_dim,
            generators=self.generators
        )

        # Position priors for each layer (Î¼, Ïƒ only - NO Ï†!)
        self.position_priors = nn.ModuleList([
            PositionPriors(config.max_seq_len, config.embed_dim)
            for _ in range(config.n_layers)
        ])

    def forward(self, input_ids, target_ids=None):
        B, N = input_ids.shape
        device = input_ids.device

        # === ENCODING ===
        # Initialize (Î¼_q, Ïƒ_q, Ï†) from token priors
        Î¼_q, Ïƒ_q, Ï† = self.token_priors.encode(input_ids)

        # Causal mask
        mask = torch.tril(torch.ones(N, N, device=device, dtype=torch.bool))

        # === LAYERS ===
        for layer_idx in range(self.config.n_layers):
            Î¼_p = self.position_priors[layer_idx].Î¼_p[:N]
            Ïƒ_p = self.position_priors[layer_idx].Ïƒ_p[:N]

            # Q-flow with gauge evolution
            Î¼_q, Ïƒ_q, Ï† = self.q_flow(
                Î¼_q, Ïƒ_q, Ï†, Î¼_p, Ïƒ_p, mask, target_ids
            )

        # === DECODING ===
        logits = self.token_priors.decode(Î¼_q, Ïƒ_q, Ï†)

        loss = None
        if target_ids is not None:
            loss = F.cross_entropy(
                logits.view(-1, self.config.vocab_size),
                target_ids.view(-1)
            )

        return logits, loss

    def q_flow(self, Î¼_q, Ïƒ_q, Ï†, Î¼_p, Ïƒ_p, mask, targets):
        """VFE gradient descent on beliefs AND gauge frames."""
        for step in range(self.config.n_vfe_steps):
            Î¼_q, Ïƒ_q, Ï† = vfe_step(
                Î¼_q, Ïƒ_q, Ï†, Î¼_p, Ïƒ_p,
                self.generators_buf, mask, targets,
                self.token_priors, self.config
            )
        return Î¼_q, Ïƒ_q, Ï†
```

---

## VIII. What We KEEP vs AVOID

### KEEP (Core FEP with Gauge Structure)

| Component | Role |
|-----------|------|
| Gauge frames Ï† | Semantic/feature encoding |
| Transport Î©_ij | Frame alignment for comparison |
| KL(q_i \|\| Î©_ijÂ·q_j) | Transported belief alignment |
| KL(p_i \|\| Î©_ijÂ·p_j) | Prior coupling (world model coherence) |
| âˆ‚F/âˆ‚Ï† | Gauge frame evolution |
| Multi-head from dim(ğ”¤) | Natural head structure |

### AVOID (Ad Hoc / Neural)

| Eliminated | Reason |
|------------|--------|
| Position in Ï† | Ï† is for semantics, not position |
| Sinusoidal encoding | Position emerges from priors |
| W_Q, W_K, W_V matrices | Attention from KL geometry |
| MLPs / FFN | VFE gradient descent |
| GELU/ReLU | Softmax gradient nonlinearity |
| Learned projections | All from VFE |

---

## IX. Implementation Phases (Revised)

### Phase 1: Gauge Infrastructure (Week 1)
1. Implement SO(3) generators
2. Implement transport operator computation
3. Implement transported KL divergence
4. Test gauge equivariance properties

### Phase 2: Complete VFE (Week 2)
1. Implement all four VFE terms
2. Implement gradient computation (including âˆ‚F/âˆ‚Ï†)
3. Validate gradients with finite differences
4. Test on simple examples

### Phase 3: Token & Position Priors (Week 3)
1. Implement TokenPriorBank with Ï†_tokens
2. Implement PositionPriors (Î¼, Ïƒ only)
3. Encoding/decoding with transport
4. P-flow updates

### Phase 4: Full Model (Week 4)
1. Stack layers
2. Training loop with Q-flow + P-flow
3. WikiText-2 experiments
4. Compare to standard transformer

### Phase 5: Analysis (Week 5)
1. Visualize learned Ï† structure
2. Analyze attention patterns
3. Study semantic clustering
4. Multi-head decomposition

---

## X. Key Equations Summary (REVISED)

| Component | Equation |
|-----------|----------|
| **Agent** | (q_i, p_i, Ï†_i) = (N(Î¼_qi, Ïƒ_qiÂ²), N(Î¼_pi, Ïƒ_piÂ²), Ï†_i âˆˆ ğ”¤) |
| **Transport** | Î©_ij = exp(Ï†_iÂ·G) Â· exp(-Ï†_jÂ·G) |
| **Transported Mean** | Î¼Ìƒ_j = Î©_ij Â· Î¼_j |
| **Transported Var** | ÏƒÌƒ_jÂ² = diag(Î©_ij Â· diag(Ïƒ_jÂ²) Â· Î©_ij^T) |
| **Belief Attention** | Î²_ij = softmax_j(-KL(q_i \|\| Î©_ijÂ·q_j) / Îº_Î²) |
| **Prior Attention** | Î³_ij = softmax_j(-KL(p_i \|\| Î©_ijÂ·p_j) / Îº_Î³) |
| **VFE** | F = Î±Â·Î£ KL(q\|\|p) + Î»_Î²Â·Î£ Î²Â·KL(q\|\|Î©Â·q) + Î»_Î³Â·Î£ Î³Â·KL(p\|\|Î©Â·p) - log p(y) |
| **Natural Gradient Î¼** | Î¼ â† Î¼ - Î·_Î¼ Â· ÏƒÂ² Â· âˆ‚F/âˆ‚Î¼ |
| **Gauge Update** | Ï† â† Ï† - Î·_Ï† Â· âˆ‚F/âˆ‚Ï† |

---

*Revised plan incorporating gauge frames as CORE semantic encoding mechanism.*
*Ï† encodes WHAT (semantics), priors encode WHERE (position).*
*Full transport Î©_ij in ALL KL terms including prior coupling.*
